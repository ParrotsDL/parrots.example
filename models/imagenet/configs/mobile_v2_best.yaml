seed: 99
net:
    arch: mobile_v2
    kwargs:
        num_classes: 1000
        scale: 1.0
 
dataset:
  train:
    meta_file: /mnt/lustre/share/images/meta/train.txt
    image_dir: /mnt/lustre/share/images/train
    random_resize_crop: 224
    colorjitter: [0.2, 0.2, 0.2, 0.1]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: True
  test:
    meta_file: /mnt/lustre/share/images/meta/val.txt
    image_dir: /mnt/lustre/share/images/val
    resize: 256
    center_crop: [224, 224]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: False
  iter_wise: True
  batch_size: 64
  workers: 4
 
trainer:
  max_iter: 500000
  test_freq: 1000
  log_freq: 100
  label_smooth: 0.1
  bn:
    syncbn: False
    weight_decay: False
  ema:
    enable: true
    kwargs:
      decay: 0.999
  mixed_precision:
    half: False
    loss_scale: 128.0
    float_bn: True # define floating layers when mixed training
      # loss:
      #       #     float_out: True
      #
  optimizer:
    type: SGD
    kwargs:
      lr: 0.1
      momentum: 0.9
      nesterov: True
      weight_decay: 0.00004
  lr_scheduler:
    type: CosineLR
    kwargs:
      max_iter: 500000
      min_lr: 0.0
    
      base_lr: 0.1
      warmup_lr: 0.4
      warmup_steps: 5000

saver:
  pretrian_model: 
  resume_model:
  save_dir: checkpoints/mobile_v2   # save checkpoint locally
 
monitor:
  type: pavi
  kwargs:
    taskid:
    project: default
    task: mobile_v2_pt1.2_prototype_bs64_ema
    model: mobile_v2_1x
