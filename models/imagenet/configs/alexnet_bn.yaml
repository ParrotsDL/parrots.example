seed: 99
net:
    arch: alexnet_bn
    kwargs:
        num_classes: 1000
 
dataset:
  dummy: True
  train:
    meta_file: /mnt/lustre/share/images/meta/train.txt
    image_dir: /mnt/lustre/share/images/train
    reader: pillow
    # resize: 256
    # center_crop: [224, 224]
    # random_crop: [224, 224]
    random_resize_crop: 224
    # colorjitter: [0.2, 0.2, 0.2, 0.1]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: True
  test:
    meta_file: /mnt/lustre/share/images/meta/val.txt
    image_dir: /mnt/lustre/share/images/val
    reader: pillow
    resize: 256
    center_crop: [224, 224] 
    # colorjitter:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: False
  batch_size: 512
  workers: 4
 
trainer:
  max_epoch: 100
  test_freq: 10
  log_freq: 100
  mixed_training:
    half: False
    loss_scale: 128.0
  #   float_layers:  # define floating layers when mixed training
      # loss:
      #     float_out: True
  # sparse:
  #   type: False
  #   dropratio: 0.7
  #   warmup_iters: 100
  optimizer:
    type: LARS
    kwargs:
      lr: 5
      momentum: 0.9
      weight_decay: 0.0005
      eta: 0.001
      # sparse: False
  lr_scheduler:
    warmup_epochs: 5
    base_lr: 0.5
    warmup_lr: 5
    type: PolyLR
    kwargs:
      power: 2
 
saver:
  pretrain_model: 
  resume_model: 
  test_model: resnet50_ckpt_e56304.pth #e28152.pth
  load_pavi:   # load pretrain/resume model from pavi
  save_pavi: False   # add snapshot to pavi
  save_dir: checkpoints   # save checkpoint locally
  save_epoch_freq: 2
  save_latest: False
  save_best: False
  sync_bn_stats: False  
 
monitor:
  # type: pavi
  # _taskid:  # continue training
  # kwargs:
  #   project: alexnet1K  # change to your own pavi project
  #   task: alexnet_sparse0.5
  #   model: alexnet_bn
  type: tensorboard
  kwargs:
      logdir: tensorboard/alexnet
