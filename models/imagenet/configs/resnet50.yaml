seed: 99
net:
    arch: resnet50 # resnet18, resnet34, resnet50, resnet50c, resnet50d, resnet101, resnet101d, resnet152, resnet152d
                   # resnext50_32x4d, resnext101_32x8d, resnext50d_32x4d, resnext101d_32x8d
                   # wide_resnet50_2, wide_resnet101_2, wide_resnet50d_2, wide_resnet101d_2
    kwargs:
        num_classes: 1000
 
dataset:
  train:
    meta_file: /mnt/lustre/share/images/meta/train.txt
    image_dir: /mnt/lustre/share/images/train
    resize: 224
    random_resize_crop: 224
    # colorjitter: [0.2, 0.2, 0.2, 0.1]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: True
  test:
    meta_file: /mnt/lustre/share/images/meta/val.txt
    image_dir: /mnt/lustre/share/images/val
    resize: 224
    center_crop: [224, 224]
    colorjitter:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    mirror: False
  batch_size: 256 # resnet101 < 128
  workers: 4
 
trainer:
  max_epoch: 100
  test_freq: 5
  log_freq: 100
  bn:
    syncbn: False
  mixed_precision:
    half: False
    loss_scale: 128.0
    float_bn: True
    #float_module_type: "{torch.nn.Conv2d:('float', 'half')}"
    #float_module_name: "{'fc':('float', 'half')}"
  optimizer:
    type: SGD
    kwargs:
      lr: 6.4
      momentum: 0.9
      weight_decay: 0.0005
  lr_scheduler:
    warmup_epochs: 5
    base_lr: 0.64
    warmup_lr: 6.4
    type: PolyLR
    kwargs:
      power: 2


saver:
  pretrain_model:
  resume_model:
  save_dir: checkpoints/resnet50   # save checkpoint locally
 
monitor:
  # type: pavi
#   _taskid: # continue training
#   kwargs:
#     project: pape_test  # change to your own pavi project
#     task: resnet50
#     model: resnet50
  type: tensorboard
  kwargs:
      logdir: tensorboard/poly1k_lr0.8
