--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:22,712 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 2 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 3 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 4 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 5 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 7 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 1 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 6 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:24,625 [32mINFO[0m: rank 0 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 14:58:30,541 [32mINFO[0m: config
{
  "seed": 99,
  "net": {
    "arch": "resnet50",
    "kwargs": {
      "num_classes": 1000
    }
  },
  "dataset": {
    "train": {
      "meta_file": "/mnt/lustre/share/images/meta/train.txt",
      "image_dir": "/mnt/lustre/share/images/train",
      "random_resize_crop": 224,
      "colorjitter": [
        0.2,
        0.2,
        0.2,
        0.1
      ],
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": true
    },
    "test": {
      "meta_file": "/mnt/lustre/share/images/meta/val.txt",
      "image_dir": "/mnt/lustre/share/images/val",
      "resize": 256,
      "center_crop": [
        224,
        224
      ],
      "colorjitter": null,
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": false
    },
    "batch_size": 32,
    "workers": 4
  },
  "trainer": {
    "max_epoch": 100,
    "test_freq": 1,
    "log_freq": 20,
    "bn": {
      "syncbn": false
    },
    "mixed_precision": {
      "half": false,
      "loss_scale": 128.0,
      "float_bn": true
    },
    "optimizer": {
      "type": "SGD",
      "kwargs": {
        "lr": 0.1,
        "momentum": 0.9,
        "weight_decay": 0.0001
      }
    },
    "lr_scheduler": {
      "warmup_epochs": 0,
      "type": "MultiStepLR",
      "kwargs": {
        "milestones": [
          30,
          60,
          90
        ],
        "gamma": 0.1
      }
    }
  },
  "saver": {
    "pretrain_model": null,
    "resume_model": null,
    "save_dir": "checkpoints/resnet50"
  },
  "monitor": {
    "type": "pavi",
    "_taskid": null,
    "kwargs": {
      "project": "pape_test",
      "task": "resnet50",
      "model": "resnet50"
    }
  }
}[0m
2020-07-02 14:58:30,867 [32mINFO[0m: creating model 'resnet50'[0m
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: bn1
convert module: relu
convert module: relu
convert module: relu
convert module: relu
convert module: relu
convert module: relu
convert module: relu
convert module: relu
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: maxpool
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer1.0.bn1
convert module: layer1.0.conv1
convert module: layer1.0.bn1
convert module: layer1.0.bn1
convert module: layer1.0.bn1
convert module: layer1.0.bn1
convert module: layer1.0.bn1
convert module: layer1.0.conv2
convert module: layer1.0.bn1
convert module: layer1.0.conv2
convert module: layer1.0.conv2
convert module: layer1.0.bn1
convert module: layer1.0.conv2
convert module: layer1.0.conv2
convert module: layer1.0.bn2
convert module: layer1.0.conv2
convert module: layer1.0.conv2
convert module: layer1.0.bn2
convert module: layer1.0.conv2
convert module: layer1.0.bn2
convert module: layer1.0.bn2
convert module: layer1.0.conv3
convert module: layer1.0.conv3
convert module: layer1.0.conv3
convert module: layer1.0.bn2
convert module: layer1.0.conv3
convert module: layer1.0.bn2
convert module: layer1.0.bn2
convert module: layer1.0.bn3
convert module: layer1.0.bn2
convert module: layer1.0.bn3
convert module: layer1.0.bn3
convert module: layer1.0.bn3
convert module: layer1.0.conv3
convert module: layer1.0.conv3
convert module: layer1.0.conv3
convert module: layer1.0.relu
convert module: layer1.0.conv3
convert module: layer1.0.relu
convert module: layer1.0.relu
convert module: layer1.0.relu
convert module: layer1.0.downsample.0
convert module: layer1.0.bn3
convert module: layer1.0.bn3
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.0
convert module: layer1.0.bn3
convert module: layer1.0.downsample.0
convert module: layer1.0.bn3
convert module: layer1.0.downsample.1
convert module: layer1.0.relu
convert module: layer1.0.downsample.1
convert module: layer1.0.downsample.1
convert module: layer1.0.downsample.1
convert module: layer1.0.relu
convert module: layer1.0.relu
convert module: layer1.0.relu
convert module: layer1.1.conv1
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.0
convert module: layer1.1.conv1
convert module: layer1.1.conv1
convert module: layer1.1.conv1
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.0
convert module: layer1.1.bn1
convert module: layer1.1.bn1
convert module: layer1.0.downsample.1
convert module: layer1.1.bn1
convert module: layer1.0.downsample.1
convert module: layer1.1.bn1
convert module: layer1.0.downsample.1
convert module: layer1.0.downsample.1
convert module: layer1.1.conv2
convert module: layer1.1.conv2
convert module: layer1.1.conv2
convert module: layer1.1.conv1
convert module: layer1.1.conv2
convert module: layer1.1.bn2
convert module: layer1.1.conv1
convert module: layer1.1.conv1
convert module: layer1.1.conv1
convert module: layer1.1.bn2
convert module: layer1.1.bn2
convert module: layer1.1.bn2
convert module: layer1.1.conv3
convert module: layer1.1.bn1
convert module: layer1.1.bn1
convert module: layer1.1.conv3
convert module: layer1.1.bn1
convert module: layer1.1.bn1
convert module: layer1.1.conv3
convert module: layer1.1.conv3
convert module: layer1.1.bn3
convert module: layer1.1.conv2
convert module: layer1.1.bn3
convert module: layer1.1.bn3
convert module: layer1.1.bn3
convert module: layer1.1.conv2
convert module: layer1.1.conv2
convert module: layer1.1.relu
convert module: layer1.1.conv2
convert module: layer1.1.relu
convert module: layer1.1.bn2
convert module: layer1.1.relu
convert module: layer1.1.relu
convert module: layer1.2.conv1
convert module: layer1.1.bn2
convert module: layer1.2.conv1
convert module: layer1.1.bn2
convert module: layer1.1.bn2
convert module: layer1.2.conv1
convert module: layer1.2.conv1
convert module: layer1.2.bn1
convert module: layer1.1.conv3
convert module: layer1.1.conv3
convert module: layer1.2.bn1
convert module: layer1.1.conv3
convert module: layer1.2.bn1
convert module: layer1.2.bn1
convert module: layer1.2.conv2
convert module: layer1.1.conv3
convert module: layer1.2.conv2
convert module: layer1.1.bn3
convert module: layer1.1.bn3
convert module: layer1.2.conv2
convert module: layer1.2.conv2
convert module: layer1.2.bn2
convert module: layer1.1.bn3
convert module: layer1.1.bn3
convert module: layer1.2.bn2
convert module: layer1.1.relu
convert module: layer1.2.bn2
convert module: layer1.2.conv3
convert module: layer1.1.relu
convert module: layer1.2.bn2
convert module: layer1.1.relu
convert module: layer1.1.relu
convert module: layer1.2.conv3
convert module: layer1.2.conv1
convert module: layer1.2.conv3
convert module: layer1.2.bn3
convert module: layer1.2.conv1
convert module: layer1.2.conv3
convert module: layer1.2.conv1
convert module: layer1.2.conv1
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer1.2.bn1
convert module: layer1.2.bn1
convert module: layer1.2.relu
convert module: layer1.2.bn1
convert module: layer1.2.bn1
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: layer1.2.conv2
convert module: layer1.2.conv2
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: layer1.2.conv2
convert module: layer1.2.relu
convert module: layer1.2.conv2
convert module: layer2.0.bn1
convert module: layer1.2.bn2
convert module: layer2.0.conv1
convert module: layer2.0.conv1
convert module: layer1.2.bn2
convert module: layer2.0.bn1
convert module: layer1.2.bn2
convert module: layer2.0.conv2
convert module: layer1.2.bn2
convert module: layer2.0.bn1
convert module: layer2.0.bn1
convert module: layer1.2.conv3
convert module: layer2.0.conv2
convert module: layer1.2.conv3
convert module: layer1.2.conv3
convert module: layer1.2.conv3
convert module: layer2.0.conv2
convert module: layer2.0.conv2
convert module: layer2.0.bn2
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer2.0.conv3
convert module: layer2.0.bn2
convert module: layer2.0.bn2
convert module: layer2.0.bn2
convert module: layer1.2.relu
convert module: layer1.2.relu
convert module: layer2.0.bn3
convert module: layer2.0.conv3
convert module: layer2.0.conv3
convert module: layer2.0.conv3
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: layer2.0.relu
convert module: layer2.0.bn3
convert module: layer2.0.bn3
convert module: layer2.0.conv1
convert module: layer2.0.bn3
convert module: layer2.0.conv1
convert module: layer2.0.downsample.0
convert module: layer2.0.bn1
convert module: layer2.0.relu
convert module: layer2.0.relu
convert module: layer2.0.relu
convert module: layer2.0.bn1
convert module: layer2.0.bn1
convert module: layer2.0.bn1
convert module: layer2.0.downsample.0
convert module: layer2.0.conv2
convert module: layer2.0.downsample.0
convert module: layer2.0.downsample.1
convert module: layer2.0.downsample.0
convert module: layer2.0.conv2
convert module: layer2.0.conv2
convert module: layer2.1.conv1
convert module: layer2.0.conv2
convert module: layer2.0.downsample.1
convert module: layer2.0.downsample.1
convert module: layer2.0.bn2
convert module: layer2.0.downsample.1
convert module: layer2.1.bn1
convert module: layer2.0.bn2
convert module: layer2.1.conv1
convert module: layer2.1.conv1
convert module: layer2.0.bn2
convert module: layer2.0.conv3
convert module: layer2.0.bn2
convert module: layer2.1.conv2
convert module: layer2.1.conv1
convert module: layer2.0.conv3
convert module: layer2.1.bn1
convert module: layer2.1.bn1
convert module: layer2.0.conv3
convert module: layer2.0.conv3
convert module: layer2.0.bn3
convert module: layer2.1.bn1
convert module: layer2.1.conv2
convert module: layer2.1.conv2
convert module: layer2.1.bn2
convert module: layer2.0.relu
convert module: layer2.0.bn3
convert module: layer2.1.conv2
convert module: layer2.0.bn3
convert module: layer2.1.conv3
convert module: layer2.0.bn3
convert module: layer2.0.downsample.0
convert module: layer2.1.bn2
convert module: layer2.1.bn2
convert module: layer2.0.relu
convert module: layer2.0.relu
convert module: layer2.1.bn3
convert module: layer2.1.bn2
convert module: layer2.1.conv3
convert module: layer2.0.downsample.0
convert module: layer2.0.relu
convert module: layer2.1.conv3
convert module: layer2.1.relu
convert module: layer2.0.downsample.0
convert module: layer2.0.downsample.0
convert module: layer2.1.conv3
convert module: layer2.1.bn3
convert module: layer2.0.downsample.1
convert module: layer2.1.bn3
convert module: layer2.2.conv1
convert module: layer2.1.bn3
convert module: layer2.1.relu
convert module: layer2.1.relu
convert module: layer2.0.downsample.1
convert module: layer2.0.downsample.1
convert module: layer2.1.conv1
convert module: layer2.2.bn1
convert module: layer2.0.downsample.1
convert module: layer2.2.conv1
convert module: layer2.2.conv1
convert module: layer2.1.relu
convert module: layer2.1.conv1
convert module: layer2.2.conv2
convert module: layer2.1.conv1
convert module: layer2.1.bn1
convert module: layer2.2.conv1
convert module: layer2.2.bn1
convert module: layer2.1.conv1
convert module: layer2.2.bn1
convert module: layer2.1.bn1
convert module: layer2.1.bn1
convert module: layer2.2.conv2
convert module: layer2.2.bn1
convert module: layer2.2.conv2
convert module: layer2.2.bn2
convert module: layer2.1.conv2
convert module: layer2.1.bn1
convert module: layer2.1.conv2
convert module: layer2.1.conv2
convert module: layer2.2.conv2
convert module: layer2.2.conv3
convert module: layer2.1.conv2
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer2.2.bn3
convert module: layer2.1.bn2
convert module: layer2.2.conv3
convert module: layer2.1.bn2
convert module: layer2.2.conv3
convert module: layer2.2.bn2
convert module: layer2.1.bn2
convert module: layer2.1.bn2
convert module: layer2.2.relu
convert module: layer2.1.conv3
convert module: layer2.2.bn3
convert module: layer2.2.conv3
convert module: layer2.1.conv3
convert module: layer2.2.bn3
convert module: layer2.3.conv1
convert module: layer2.1.conv3
convert module: layer2.1.conv3
convert module: layer2.2.relu
convert module: layer2.1.bn3
convert module: layer2.2.relu
convert module: layer2.2.bn3
convert module: layer2.3.bn1
convert module: layer2.3.conv1
convert module: layer2.1.bn3
convert module: layer2.1.bn3
convert module: layer2.3.conv1
convert module: layer2.1.bn3
convert module: layer2.2.relu
convert module: layer2.1.relu
convert module: layer2.3.conv2
convert module: layer2.3.bn1
convert module: layer2.1.relu
convert module: layer2.1.relu
convert module: layer2.3.conv1
convert module: layer2.3.bn1
convert module: layer2.1.relu
convert module: layer2.2.conv1
convert module: layer2.2.conv1
convert module: layer2.3.conv2
convert module: layer2.2.conv1
convert module: layer2.3.conv2
convert module: layer2.3.bn2
convert module: layer2.2.conv1
convert module: layer2.3.bn1
convert module: layer2.2.bn1
convert module: layer2.2.bn1
convert module: layer2.2.bn1
convert module: layer2.3.conv3
convert module: layer2.3.conv2
convert module: layer2.3.bn2
convert module: layer2.2.bn1
convert module: layer2.3.bn2
convert module: layer2.2.conv2
convert module: layer2.2.conv2
convert module: layer2.3.conv3
convert module: layer2.2.conv2
convert module: layer2.3.bn3
convert module: layer2.3.conv3
convert module: layer2.2.conv2
convert module: layer2.3.relu
convert module: layer2.3.bn2
convert module: layer2.3.bn3
convert module: layer2.2.bn2
convert module: layer2.3.bn3
convert module: layer2.2.bn2
convert module: layer3.0.conv1
convert module: layer2.3.conv3
convert module: layer2.3.relu
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer2.2.conv3
convert module: layer2.3.relu
convert module: layer2.2.conv3
convert module: layer3.0.conv1
convert module: layer2.2.conv3
convert module: layer2.3.bn3
convert module: layer2.2.conv3
convert module: layer3.0.bn1
convert module: layer3.0.conv1
convert module: layer2.2.bn3
convert module: layer2.3.relu
convert module: layer2.2.bn3
convert module: layer3.0.conv2
convert module: layer3.0.bn1
convert module: layer2.2.bn3
convert module: layer2.2.bn3
convert module: layer3.0.conv1
convert module: layer3.0.bn1
convert module: layer2.2.relu
convert module: layer2.2.relu
convert module: layer3.0.conv2
convert module: layer2.2.relu
convert module: layer3.0.conv2
convert module: layer2.3.conv1
convert module: layer2.2.relu
convert module: layer2.3.conv1
convert module: layer2.3.conv1
convert module: layer3.0.bn1
convert module: layer2.3.conv1
convert module: layer3.0.bn2
convert module: layer2.3.bn1
convert module: layer3.0.conv2
convert module: layer2.3.bn1
convert module: layer3.0.conv3
convert module: layer3.0.bn2
convert module: layer2.3.bn1
convert module: layer3.0.bn2
convert module: layer2.3.conv2
convert module: layer2.3.bn1
convert module: layer2.3.conv2
convert module: layer3.0.conv3
convert module: layer2.3.conv2
convert module: layer3.0.conv3
convert module: layer3.0.bn3
convert module: layer3.0.bn2
convert module: layer2.3.conv2
convert module: layer2.3.bn2
convert module: layer2.3.bn2
convert module: layer3.0.relu
convert module: layer3.0.bn3
convert module: layer3.0.conv3
convert module: layer2.3.bn2
convert module: layer3.0.bn3
convert module: layer3.0.downsample.0
convert module: layer2.3.conv3
convert module: layer3.0.relu
convert module: layer2.3.bn2
convert module: layer3.0.relu
convert module: layer2.3.conv3
convert module: layer3.0.downsample.0
convert module: layer2.3.conv3
convert module: layer3.0.downsample.0
convert module: layer2.3.conv3
convert module: layer2.3.bn3
convert module: layer3.0.bn3
convert module: layer2.3.bn3
convert module: layer3.0.downsample.1
convert module: layer2.3.bn3
convert module: layer2.3.relu
convert module: layer3.0.relu
convert module: layer2.3.relu
convert module: layer2.3.bn3
convert module: layer3.0.downsample.1
convert module: layer3.0.downsample.1
convert module: layer2.3.relu
convert module: layer3.0.downsample.0
convert module: layer3.0.conv1
convert module: layer3.1.conv1
convert module: layer3.0.conv1
convert module: layer2.3.relu
convert module: layer3.0.conv1
convert module: layer3.1.conv1
convert module: layer3.1.conv1
convert module: layer3.0.conv1
convert module: layer3.0.bn1
convert module: layer3.0.bn1
convert module: layer3.1.bn1
convert module: layer3.0.downsample.1
convert module: layer3.1.bn1
convert module: layer3.1.bn1
convert module: layer3.0.conv2
convert module: layer3.0.bn1
convert module: layer3.0.conv2
convert module: layer3.1.conv2
convert module: layer3.1.conv1
convert module: layer3.0.bn1
convert module: layer3.1.conv2
convert module: layer3.1.conv2
convert module: layer3.0.conv2
convert module: layer3.0.conv2
convert module: layer3.1.bn1
convert module: layer3.1.bn2
convert module: layer3.0.bn2
convert module: layer3.1.bn2
convert module: layer3.0.bn2
convert module: layer3.1.bn2
convert module: layer3.1.conv2
convert module: layer3.0.bn2
convert module: layer3.1.conv3
convert module: layer3.1.conv3
convert module: layer3.0.conv3
convert module: layer3.1.conv3
convert module: layer3.0.bn2
convert module: layer3.0.conv3
convert module: layer3.0.conv3
convert module: layer3.0.conv3
convert module: layer3.1.bn3
convert module: layer3.1.bn3
convert module: layer3.0.bn3
convert module: layer3.1.bn2
convert module: layer3.1.bn3
convert module: layer3.0.bn3
convert module: layer3.1.relu
convert module: layer3.1.relu
convert module: layer3.1.conv3
convert module: layer3.1.relu
convert module: layer3.0.relu
convert module: layer3.0.relu
convert module: layer3.0.bn3
convert module: layer3.2.conv1
convert module: layer3.2.conv1
convert module: layer3.2.conv1
convert module: layer3.0.bn3
convert module: layer3.0.downsample.0
convert module: layer3.0.downsample.0
convert module: layer3.0.relu
convert module: layer3.0.relu
convert module: layer3.0.downsample.0
convert module: layer3.1.bn3
convert module: layer3.2.bn1
convert module: layer3.2.bn1
convert module: layer3.0.downsample.0
convert module: layer3.2.bn1
convert module: layer3.2.conv2
convert module: layer3.2.conv2
convert module: layer3.0.downsample.1
convert module: layer3.1.relu
convert module: layer3.0.downsample.1
convert module: layer3.2.conv2
convert module: layer3.2.conv1
convert module: layer3.1.conv1
convert module: layer3.0.downsample.1
convert module: layer3.1.conv1
convert module: layer3.0.downsample.1
convert module: layer3.2.bn2
convert module: layer3.2.bn2
convert module: layer3.1.conv1
convert module: layer3.2.bn1
convert module: layer3.1.conv1
convert module: layer3.2.bn2
convert module: layer3.1.bn1
convert module: layer3.2.conv3
convert module: layer3.2.conv3
convert module: layer3.1.bn1
convert module: layer3.2.conv3
convert module: layer3.2.conv2
convert module: layer3.1.conv2
convert module: layer3.1.bn1
convert module: layer3.1.bn1
convert module: layer3.1.conv2
convert module: layer3.2.bn3
convert module: layer3.2.bn3
convert module: layer3.2.bn3
convert module: layer3.1.conv2
convert module: layer3.1.conv2
convert module: layer3.2.relu
convert module: layer3.2.relu
convert module: layer3.2.bn2
convert module: layer3.2.relu
convert module: layer3.3.conv1
convert module: layer3.1.bn2
convert module: layer3.3.conv1
convert module: layer3.3.conv1
convert module: layer3.2.conv3
convert module: layer3.1.bn2
convert module: layer3.1.conv3
convert module: layer3.1.bn2
convert module: layer3.3.bn1
convert module: layer3.1.conv3
convert module: layer3.1.bn2
convert module: layer3.3.bn1
convert module: layer3.3.conv2
convert module: layer3.3.bn1
convert module: layer3.2.bn3
convert module: layer3.1.conv3
convert module: layer3.3.conv2
convert module: layer3.1.conv3
convert module: layer3.1.bn3
convert module: layer3.3.conv2
convert module: layer3.1.bn3
convert module: layer3.2.relu
convert module: layer3.1.bn3
convert module: layer3.1.bn3
convert module: layer3.3.conv1
convert module: layer3.1.relu
convert module: layer3.3.bn2
convert module: layer3.1.relu
convert module: layer3.2.conv1
convert module: layer3.3.bn2
convert module: layer3.1.relu
convert module: layer3.2.conv1
convert module: layer3.1.relu
convert module: layer3.3.conv3
convert module: layer3.3.bn2
convert module: layer3.3.conv3
convert module: layer3.3.bn1
convert module: layer3.2.conv1
convert module: layer3.2.conv1
convert module: layer3.3.conv3
convert module: layer3.3.bn3
convert module: layer3.2.bn1
convert module: layer3.2.bn1
convert module: layer3.3.conv2
convert module: layer3.3.bn3
convert module: layer3.3.relu
convert module: layer3.2.bn1
convert module: layer3.2.bn1
convert module: layer3.3.bn3
convert module: layer3.2.conv2
convert module: layer3.2.conv2
convert module: layer3.3.relu
convert module: layer3.4.conv1
convert module: layer3.2.conv2
convert module: layer3.3.relu
convert module: layer3.2.conv2
convert module: layer3.4.conv1
convert module: layer3.3.bn2
convert module: layer3.4.conv1
convert module: layer3.3.conv3
convert module: layer3.2.bn2
convert module: layer3.4.bn1
convert module: layer3.2.bn2
convert module: layer3.2.bn2
convert module: layer3.4.bn1
convert module: layer3.4.conv2
convert module: layer3.4.bn1
convert module: layer3.2.conv3
convert module: layer3.2.conv3
convert module: layer3.2.conv3
convert module: layer3.2.bn2
convert module: layer3.4.conv2
convert module: layer3.3.bn3
convert module: layer3.4.conv2
convert module: layer3.3.relu
convert module: layer3.2.conv3
convert module: layer3.2.bn3
convert module: layer3.4.bn2
convert module: layer3.2.bn3
convert module: layer3.4.conv1
convert module: layer3.2.bn3
convert module: layer3.4.bn2
convert module: layer3.4.conv3
convert module: layer3.2.relu
convert module: layer3.4.bn2
convert module: layer3.2.bn3
convert module: layer3.2.relu
convert module: layer3.4.conv3
convert module: layer3.2.relu
convert module: layer3.3.conv1
convert module: layer3.4.conv3
convert module: layer3.3.conv1
convert module: layer3.2.relu
convert module: layer3.4.bn3
convert module: layer3.4.bn1
convert module: layer3.3.conv1
convert module: layer3.4.relu
convert module: layer3.4.bn3
convert module: layer3.3.conv1
convert module: layer3.4.conv2
convert module: layer3.4.bn3
convert module: layer3.3.bn1
convert module: layer3.5.conv1
convert module: layer3.3.bn1
convert module: layer3.4.relu
convert module: layer3.4.relu
convert module: layer3.5.conv1
convert module: layer3.3.bn1
convert module: layer3.5.conv1
convert module: layer3.3.conv2
convert module: layer3.3.bn1
convert module: layer3.3.conv2
convert module: layer3.5.bn1
convert module: layer3.3.conv2
convert module: layer3.4.bn2
convert module: layer3.3.conv2
convert module: layer3.5.conv2
convert module: layer3.5.bn1
convert module: layer3.5.bn1
convert module: layer3.4.conv3
convert module: layer3.5.conv2
convert module: layer3.5.conv2
convert module: layer3.3.bn2
convert module: layer3.3.bn2
convert module: layer3.3.bn2
convert module: layer3.3.conv3
convert module: layer3.5.bn2
convert module: layer3.4.bn3
convert module: layer3.3.conv3
convert module: layer3.3.bn2
convert module: layer3.3.conv3
convert module: layer3.5.conv3
convert module: layer3.5.bn2
convert module: layer3.4.relu
convert module: layer3.5.bn2
convert module: layer3.3.conv3
convert module: layer3.5.conv3
convert module: layer3.3.bn3
convert module: layer3.5.conv1
convert module: layer3.3.bn3
convert module: layer3.5.conv3
convert module: layer3.5.bn3
convert module: layer3.3.bn3
convert module: layer3.3.relu
convert module: layer3.3.relu
convert module: layer3.5.relu
convert module: layer3.3.bn3
convert module: layer3.5.bn3
convert module: layer3.5.bn1
convert module: layer3.4.conv1
convert module: layer3.5.bn3
convert module: layer3.4.conv1
convert module: layer3.3.relu
convert module: layer4.0.conv1
convert module: layer3.5.relu
convert module: layer3.3.relu
convert module: layer3.5.conv2
convert module: layer3.5.relu
convert module: layer4.0.conv1
convert module: layer3.4.conv1
convert module: layer3.4.conv1
convert module: layer4.0.conv1
convert module: layer4.0.bn1
convert module: layer3.4.bn1
convert module: layer3.4.bn1
convert module: layer3.5.bn2
convert module: layer4.0.conv2
convert module: layer4.0.bn1
convert module: layer3.4.conv2
convert module: layer3.4.conv2
convert module: layer3.5.conv3
convert module: layer3.4.bn1
convert module: layer4.0.bn1
convert module: layer3.4.bn1
convert module: layer4.0.conv2
convert module: layer4.0.conv2
convert module: layer3.4.conv2
convert module: layer3.4.conv2
convert module: layer3.5.bn3
convert module: layer3.4.bn2
convert module: layer3.4.bn2
convert module: layer3.5.relu
convert module: layer3.4.conv3
convert module: layer4.0.conv1
convert module: layer3.4.conv3
convert module: layer3.4.bn2
convert module: layer3.4.bn2
convert module: layer4.0.bn2
convert module: layer3.4.conv3
convert module: layer3.4.conv3
convert module: layer4.0.conv3
convert module: layer3.4.bn3
convert module: layer3.4.bn3
convert module: layer4.0.bn2
convert module: layer4.0.bn1
convert module: layer3.4.relu
convert module: layer3.4.relu
convert module: layer4.0.conv3
convert module: layer4.0.conv2
convert module: layer3.4.bn3
convert module: layer3.5.conv1
convert module: layer3.4.bn3
convert module: layer4.0.bn2
convert module: layer3.5.conv1
convert module: layer3.4.relu
convert module: layer4.0.bn3
convert module: layer4.0.conv3
convert module: layer3.4.relu
convert module: layer3.5.conv1
convert module: layer3.5.bn1
convert module: layer4.0.relu
convert module: layer3.5.bn1
convert module: layer3.5.conv1
convert module: layer4.0.bn3
convert module: layer4.0.downsample.0
convert module: layer3.5.conv2
convert module: layer3.5.conv2
convert module: layer4.0.relu
convert module: layer3.5.bn1
convert module: layer4.0.downsample.0
convert module: layer3.5.bn1
convert module: layer4.0.bn2
convert module: layer3.5.conv2
convert module: layer4.0.bn3
convert module: layer4.0.conv3
convert module: layer3.5.conv2
convert module: layer3.5.bn2
convert module: layer4.0.relu
convert module: layer3.5.bn2
convert module: layer4.0.downsample.0
convert module: layer3.5.conv3
convert module: layer4.0.downsample.1
convert module: layer3.5.bn2
convert module: layer3.5.conv3
convert module: layer3.5.bn2
convert module: layer4.1.conv1
convert module: layer4.0.bn3
convert module: layer4.0.downsample.1
convert module: layer3.5.conv3
convert module: layer3.5.conv3
convert module: layer3.5.bn3
convert module: layer3.5.bn3
convert module: layer4.1.conv1
convert module: layer4.0.relu
convert module: layer3.5.relu
convert module: layer3.5.relu
convert module: layer4.0.downsample.0
convert module: layer3.5.bn3
convert module: layer4.0.conv1
convert module: layer4.0.conv1
convert module: layer3.5.bn3
convert module: layer3.5.relu
convert module: layer4.1.bn1
convert module: layer3.5.relu
convert module: layer4.1.conv2
convert module: layer4.0.conv1
convert module: layer4.0.conv1
convert module: layer4.0.downsample.1
convert module: layer4.1.bn1
convert module: layer4.0.bn1
convert module: layer4.0.bn1
convert module: layer4.1.conv1
convert module: layer4.1.conv2
convert module: layer4.0.conv2
convert module: layer4.0.conv2
convert module: layer4.0.downsample.1
convert module: layer4.0.bn1
convert module: layer4.0.bn1
convert module: layer4.1.conv1
convert module: layer4.0.conv2
convert module: layer4.0.conv2
convert module: layer4.1.bn2
convert module: layer4.1.conv3
convert module: layer4.1.bn1
convert module: layer4.1.conv2
convert module: layer4.1.bn2
convert module: layer4.1.bn1
convert module: layer4.0.bn2
convert module: layer4.1.conv3
convert module: layer4.1.conv2
convert module: layer4.0.conv3
convert module: layer4.1.bn3
convert module: layer4.0.bn2
convert module: layer4.0.bn2
convert module: layer4.0.bn2
convert module: layer4.1.relu
convert module: layer4.0.conv3
convert module: layer4.0.conv3
convert module: layer4.0.conv3
convert module: layer4.2.conv1
convert module: layer4.1.bn3
convert module: layer4.0.bn3
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.1.bn2
convert module: layer4.0.relu
convert module: layer4.1.bn2
convert module: layer4.1.conv3
convert module: layer4.0.bn3
convert module: layer4.2.bn1
convert module: layer4.0.bn3
convert module: layer4.0.downsample.0
convert module: layer4.0.bn3
convert module: layer4.1.conv3
convert module: layer4.2.conv2
convert module: layer4.0.relu
convert module: layer4.0.relu
convert module: layer4.0.relu
convert module: layer4.2.bn1
convert module: layer4.0.downsample.0
convert module: layer4.0.downsample.0
convert module: layer4.0.downsample.0
convert module: layer4.2.conv2
convert module: layer4.1.bn3
convert module: layer4.1.relu
convert module: layer4.1.bn3
convert module: layer4.0.downsample.1
convert module: layer4.2.conv1
convert module: layer4.1.relu
convert module: layer4.2.bn2
convert module: layer4.1.conv1
convert module: layer4.2.conv1
convert module: layer4.2.conv3
convert module: layer4.0.downsample.1
convert module: layer4.2.bn2
convert module: layer4.1.conv1
convert module: layer4.2.bn1
convert module: layer4.0.downsample.1
convert module: layer4.2.conv3
convert module: layer4.0.downsample.1
convert module: layer4.2.conv2
convert module: layer4.1.conv1
convert module: layer4.1.conv1
convert module: layer4.2.bn3
convert module: layer4.1.bn1
convert module: layer4.2.bn1
convert module: layer4.2.relu
convert module: layer4.2.conv2
convert module: layer4.1.conv2
convert module: avgpool
convert module: layer4.2.bn3
convert module: layer4.1.bn1
convert module: fc
convert module: layer4.2.relu
convert module: layer4.1.conv2
convert module: avgpool
convert module: layer4.1.bn1
convert module: fc
convert module: layer4.2.bn2
convert module: layer4.1.bn1
convert module: layer4.1.conv2
convert module: layer4.2.conv3
convert module: layer4.1.conv2
convert module: layer4.1.bn2
convert module: layer4.1.conv3
convert module: layer4.2.bn2
convert module: layer4.1.bn2
convert module: layer4.2.bn3
convert module: layer4.2.conv3
convert module: layer4.2.relu
convert module: layer4.1.conv3
convert module: avgpool
convert module: layer4.1.bn3
convert module: fc
convert module: layer4.1.relu
convert module: layer4.1.bn2
convert module: layer4.2.conv1
convert module: layer4.1.bn3
convert module: layer4.2.bn3
convert module: layer4.1.bn2
convert module: layer4.1.conv3
convert module: layer4.2.relu
convert module: layer4.1.conv3
convert module: layer4.1.relu
convert module: avgpool
convert module: layer4.2.conv1
convert module: fc
convert module: layer4.2.bn1
convert module: layer4.2.conv2
convert module: layer4.1.bn3
convert module: layer4.1.bn3
convert module: layer4.2.bn1
convert module: layer4.1.relu
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.2.conv2
convert module: layer4.2.conv1
convert module: layer4.2.bn2
convert module: layer4.2.bn1
convert module: layer4.2.conv3
convert module: layer4.2.bn1
convert module: layer4.2.conv2
convert module: layer4.2.bn2
convert module: layer4.2.conv2
convert module: layer4.2.conv3
convert module: layer4.2.bn3
convert module: layer4.2.relu
convert module: layer4.2.bn3
convert module: avgpool
convert module: fc
convert module: layer4.2.relu
convert module: avgpool
convert module: layer4.2.bn2
convert module: fc
convert module: layer4.2.bn2
convert module: layer4.2.conv3
convert module: layer4.2.conv3
convert module: layer4.2.bn3
convert module: layer4.2.bn3
convert module: layer4.2.relu
convert module: layer4.2.relu
convert module: avgpool
convert module: avgpool
convert module: fc
convert module: fc
2020-07-02 14:58:39,454 [32mINFO[0m: model
DistributedParrotsModel
ResNet(
  (conv1): Conv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (bn1): BatchNorm2d(
    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (relu): ReLU(
    inplace
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (maxpool): MaxPool2d(
    kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (4): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (5): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (avgpool): AvgPool2d(
    kernel_size=7, stride=1, padding=0
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (fc): Linear(
    in_features=2048, out_features=1000, bias=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
)[0m
2020-07-02 14:58:39,455 [32mINFO[0m: loss
CrossEntropyLoss()[0m
2020-07-02 14:58:39,456 [32mINFO[0m: optimizer
SGD (
Parameter Group 0
    dampening: 0
    hold_grads: False
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)[0m
2020-07-02 14:59:00,354 [32mINFO[0m: Test: [  0/196] Time 19.283 (19.283) Loss 6.9078 (6.9078) Acc@1 0.00 (0.00) Acc@5 0.00 (0.00)[0m
2020-07-02 14:59:04,533 [32mINFO[0m: Test: [ 20/196] Time 0.205 (0.217) Loss 6.9078 (6.9078) Acc@1 0.00 (0.15) Acc@5 0.00 (0.60)[0m
2020-07-02 14:59:08,576 [32mINFO[0m: Test: [ 40/196] Time 0.217 (0.198) Loss 6.9078 (6.9078) Acc@1 0.00 (0.08) Acc@5 0.00 (0.38)[0m
2020-07-02 14:59:13,560 [32mINFO[0m: Test: [ 60/196] Time 0.214 (0.293) Loss 6.9078 (6.9078) Acc@1 0.00 (0.10) Acc@5 3.12 (0.46)[0m
2020-07-02 14:59:17,747 [32mINFO[0m: Test: [ 80/196] Time 0.214 (0.210) Loss 6.9078 (6.9078) Acc@1 0.00 (0.08) Acc@5 0.00 (0.42)[0m
2020-07-02 14:59:21,864 [32mINFO[0m: Test: [100/196] Time 0.208 (0.206) Loss 6.9078 (6.9078) Acc@1 0.00 (0.12) Acc@5 0.00 (0.50)[0m
2020-07-02 14:59:26,049 [32mINFO[0m: Test: [120/196] Time 0.198 (0.210) Loss 6.9078 (6.9078) Acc@1 0.00 (0.15) Acc@5 0.00 (0.57)[0m
2020-07-02 14:59:30,148 [32mINFO[0m: Test: [140/196] Time 0.212 (0.206) Loss 6.9078 (6.9078) Acc@1 0.00 (0.13) Acc@5 0.00 (0.53)[0m
2020-07-02 14:59:35,067 [32mINFO[0m: Test: [160/196] Time 0.217 (0.213) Loss 6.9078 (6.9078) Acc@1 0.00 (0.14) Acc@5 0.00 (0.58)[0m
2020-07-02 14:59:39,124 [32mINFO[0m: Test: [180/196] Time 0.189 (0.196) Loss 6.9078 (6.9078) Acc@1 0.00 (0.14) Acc@5 0.00 (0.55)[0m
2020-07-02 14:59:44,725 [32mINFO[0m:  Rank 0 Loss 6.9078 Acc@1 12 Acc@5 37 total_size 6250[0m
2020-07-02 14:59:44,730 [32mINFO[0m:  Rank 1 Loss 6.9078 Acc@1 8 Acc@5 36 total_size 6250[0m
2020-07-02 14:59:44,730 [32mINFO[0m:  Rank 2 Loss 6.9078 Acc@1 7 Acc@5 34 total_size 6250[0m
2020-07-02 14:59:44,730 [32mINFO[0m:  Rank 6 Loss 6.9078 Acc@1 5 Acc@5 33 total_size 6250[0m
2020-07-02 14:59:44,730 [32mINFO[0m:  Rank 3 Loss 6.9078 Acc@1 4 Acc@5 37 total_size 6250[0m
2020-07-02 14:59:44,731 [32mINFO[0m:  Rank 4 Loss 6.9078 Acc@1 3 Acc@5 19 total_size 6250[0m
2020-07-02 14:59:44,732 [32mINFO[0m:  Rank 7 Loss 6.9078 Acc@1 6 Acc@5 31 total_size 6250[0m
2020-07-02 14:59:45,054 [32mINFO[0m:  Rank 5 Loss 6.9078 Acc@1 5 Acc@5 23 total_size 6250[0m
2020-07-02 14:59:45,055 [32mINFO[0m:  * All Loss 6.9078 Acc@1 0.100 (50/50000) Acc@5 0.500 (250/50000)[0m
