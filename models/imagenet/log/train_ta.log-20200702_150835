--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,888 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:42,889 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 2 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 0 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 1 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 4 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 5 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 6 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,594 [32mINFO[0m: rank 7 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:44,595 [32mINFO[0m: rank 3 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:08:47,538 [32mINFO[0m: config
{
  "seed": 99,
  "net": {
    "arch": "resnet50",
    "kwargs": {
      "num_classes": 1000
    }
  },
  "dataset": {
    "train": {
      "meta_file": "/mnt/lustre/share/images/meta/train.txt",
      "image_dir": "/mnt/lustre/share/images/train",
      "random_resize_crop": 224,
      "colorjitter": [
        0.2,
        0.2,
        0.2,
        0.1
      ],
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": true
    },
    "test": {
      "meta_file": "/mnt/lustre/share/images/meta/val.txt",
      "image_dir": "/mnt/lustre/share/images/val",
      "resize": 256,
      "center_crop": [
        224,
        224
      ],
      "colorjitter": null,
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": false
    },
    "batch_size": 32,
    "workers": 4
  },
  "trainer": {
    "max_epoch": 100,
    "test_freq": 1,
    "log_freq": 20,
    "bn": {
      "syncbn": false
    },
    "mixed_precision": {
      "half": false,
      "loss_scale": 128.0,
      "float_bn": true
    },
    "optimizer": {
      "type": "SGD",
      "kwargs": {
        "lr": 0.1,
        "momentum": 0.9,
        "weight_decay": 0.0001
      }
    },
    "lr_scheduler": {
      "warmup_epochs": 0,
      "type": "MultiStepLR",
      "kwargs": {
        "milestones": [
          30,
          60,
          90
        ],
        "gamma": 0.1
      }
    }
  },
  "saver": {
    "pretrain_model": null,
    "resume_model": null,
    "save_dir": "checkpoints/resnet50"
  },
  "monitor": null
}[0m
2020-07-02 15:08:47,825 [32mINFO[0m: creating model 'resnet50'[0m
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: bn1
convert module: relu
convert module: maxpool
convert module: layer1.0.conv1
convert module: layer1.0.bn1
convert module: layer1.0.conv2
convert module: layer1.0.bn2
convert module: layer1.0.conv3
convert module: layer1.0.bn3
convert module: layer1.0.relu
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.1
convert module: layer1.1.conv1
convert module: layer1.1.bn1
convert module: layer1.1.conv2
convert module: layer1.1.bn2
convert module: layer1.1.conv3
convert module: layer1.1.bn3
convert module: layer1.1.relu
convert module: layer1.2.conv1
convert module: layer1.2.bn1
convert module: layer1.2.conv2
convert module: layer1.2.bn2
convert module: layer1.2.conv3
convert module: layer1.2.bn3
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: layer2.0.bn1
convert module: layer2.0.conv2
convert module: layer2.0.bn2
convert module: layer2.0.conv3
convert module: layer2.0.bn3
convert module: layer2.0.relu
convert module: layer2.0.downsample.0
convert module: layer2.0.downsample.1
convert module: layer2.1.conv1
convert module: layer2.1.bn1
convert module: layer2.1.conv2
convert module: layer2.1.bn2
convert module: layer2.1.conv3
convert module: layer2.1.bn3
convert module: layer2.1.relu
convert module: layer2.2.conv1
convert module: layer2.2.bn1
convert module: layer2.2.conv2
convert module: layer2.2.bn2
convert module: layer2.2.conv3
convert module: layer2.2.bn3
convert module: layer2.2.relu
convert module: layer2.3.conv1
convert module: layer2.3.bn1
convert module: layer2.3.conv2
convert module: bn1
convert module: layer2.3.bn2
convert module: layer2.3.conv3
convert module: bn1
convert module: relu
convert module: layer2.3.bn3
convert module: maxpool
convert module: layer2.3.relu
convert module: layer1.0.conv1
convert module: layer3.0.conv1
convert module: relu
convert module: layer1.0.bn1
convert module: layer3.0.bn1
convert module: maxpool
convert module: layer1.0.conv2
convert module: layer3.0.conv2
convert module: layer1.0.conv1
convert module: layer1.0.bn2
convert module: layer1.0.bn1
convert module: layer3.0.bn2
convert module: layer1.0.conv3
convert module: layer1.0.conv2
convert module: layer3.0.conv3
convert module: layer1.0.bn3
convert module: layer1.0.bn2
convert module: layer1.0.relu
convert module: layer3.0.bn3
convert module: layer1.0.conv3
convert module: layer1.0.downsample.0
convert module: layer3.0.relu
convert module: layer3.0.downsample.0
convert module: layer1.0.bn3
convert module: layer1.0.downsample.1
convert module: layer1.0.relu
convert module: layer1.1.conv1
convert module: bn1
convert module: bn1
convert module: layer3.0.downsample.1
convert module: layer1.0.downsample.0
convert module: layer1.1.bn1
convert module: layer3.1.conv1
convert module: relu
convert module: layer1.0.downsample.1
convert module: relu
convert module: layer1.1.conv2
convert module: maxpool
convert module: maxpool
convert module: layer1.1.conv1
convert module: layer1.1.bn2
convert module: layer3.1.bn1
convert module: layer1.0.conv1
convert module: layer1.0.conv1
convert module: layer3.1.conv2
convert module: layer1.0.bn1
convert module: layer1.0.bn1
convert module: layer1.1.conv3
convert module: layer1.1.bn1
convert module: layer1.0.conv2
convert module: layer1.0.conv2
convert module: layer1.1.conv2
convert module: layer1.1.bn3
convert module: layer1.0.bn2
convert module: layer1.0.bn2
convert module: layer3.1.bn2
convert module: layer1.1.relu
convert module: layer1.1.bn2
convert module: layer3.1.conv3
convert module: layer1.0.conv3
convert module: layer1.0.conv3
convert module: layer1.2.conv1
convert module: layer1.1.conv3
convert module: layer1.0.bn3
convert module: layer1.0.bn3
convert module: layer1.2.bn1
convert module: layer3.1.bn3
convert module: layer1.0.relu
convert module: layer1.1.bn3
convert module: layer1.0.relu
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.0
convert module: layer1.2.conv2
convert module: layer3.1.relu
convert module: layer1.1.relu
convert module: layer3.2.conv1
convert module: layer1.0.downsample.1
convert module: layer1.0.downsample.1
convert module: layer1.2.bn2
convert module: layer1.2.conv1
convert module: layer1.1.conv1
convert module: layer1.1.conv1
convert module: layer1.2.conv3
convert module: bn1
convert module: layer3.2.bn1
convert module: layer1.2.bn1
convert module: layer1.1.bn1
convert module: layer1.1.bn1
convert module: layer1.2.bn3
convert module: layer3.2.conv2
convert module: layer1.1.conv2
convert module: layer1.1.conv2
convert module: layer1.2.conv2
convert module: layer1.1.bn2
convert module: layer1.2.relu
convert module: layer1.1.bn2
convert module: layer1.2.bn2
convert module: relu
convert module: layer1.1.conv3
convert module: layer1.1.conv3
convert module: layer2.0.conv1
convert module: layer3.2.bn2
convert module: maxpool
convert module: layer1.2.conv3
convert module: layer1.1.bn3
convert module: layer1.1.bn3
convert module: layer3.2.conv3
convert module: layer1.0.conv1
convert module: layer2.0.bn1
convert module: layer1.2.bn3
convert module: layer1.1.relu
convert module: layer1.1.relu
convert module: layer1.0.bn1
convert module: layer2.0.conv2
convert module: layer1.2.conv1
convert module: layer1.2.conv1
convert module: layer3.2.bn3
convert module: layer1.0.conv2
convert module: layer1.2.bn1
convert module: layer1.2.relu
convert module: layer1.2.bn1
convert module: layer3.2.relu
convert module: bn1
convert module: layer2.0.bn2
convert module: layer1.2.conv2
convert module: layer1.2.conv2
convert module: layer2.0.conv1
convert module: layer1.0.bn2
convert module: layer3.3.conv1
convert module: layer1.2.bn2
convert module: layer1.2.bn2
convert module: layer2.0.conv3
convert module: layer2.0.bn1
convert module: relu
convert module: layer1.0.conv3
convert module: layer1.2.conv3
convert module: layer1.2.conv3
convert module: maxpool
convert module: layer3.3.bn1
convert module: layer1.0.bn3
convert module: layer2.0.conv2
convert module: bn1
convert module: layer1.2.bn3
convert module: layer1.2.bn3
convert module: layer1.0.conv1
convert module: layer2.0.bn3
convert module: layer3.3.conv2
convert module: layer1.0.relu
convert module: layer1.2.relu
convert module: layer1.0.downsample.0
convert module: layer1.0.bn1
convert module: layer2.0.relu
convert module: layer1.2.relu
convert module: layer2.0.bn2
convert module: layer2.0.conv1
convert module: layer1.0.downsample.1
convert module: layer2.0.conv1
convert module: layer2.0.downsample.0
convert module: layer1.0.conv2
convert module: layer2.0.conv3
convert module: layer3.3.bn2
convert module: relu
convert module: layer2.0.bn1
convert module: layer2.0.bn1
convert module: layer1.1.conv1
convert module: layer1.0.bn2
convert module: layer3.3.conv3
convert module: layer2.0.bn3
convert module: maxpool
convert module: layer2.0.conv2
convert module: layer2.0.conv2
convert module: layer1.1.bn1
convert module: layer2.0.downsample.1
convert module: layer1.0.conv3
convert module: layer1.0.conv1
convert module: layer1.1.conv2
convert module: layer2.0.relu
convert module: layer1.0.bn3
convert module: layer2.0.bn2
convert module: layer3.3.bn3
convert module: layer2.0.bn2
convert module: layer2.1.conv1
convert module: layer1.0.bn1
convert module: layer1.1.bn2
convert module: layer2.0.downsample.0
convert module: layer1.0.relu
convert module: layer2.0.conv3
convert module: layer3.3.relu
convert module: layer2.0.conv3
convert module: layer1.0.downsample.0
convert module: layer2.1.bn1
convert module: layer1.1.conv3
convert module: layer1.0.conv2
convert module: layer3.4.conv1
convert module: layer2.0.bn3
convert module: layer2.0.bn3
convert module: layer1.0.downsample.1
convert module: layer1.1.bn3
convert module: layer2.1.conv2
convert module: layer2.0.downsample.1
convert module: layer1.0.bn2
convert module: layer2.0.relu
convert module: layer2.0.relu
convert module: layer1.1.conv1
convert module: layer1.1.relu
convert module: layer2.1.conv1
convert module: layer2.0.downsample.0
convert module: layer1.0.conv3
convert module: layer2.0.downsample.0
convert module: layer3.4.bn1
convert module: layer1.2.conv1
convert module: layer1.1.bn1
convert module: layer2.1.bn2
convert module: layer1.0.bn3
convert module: layer3.4.conv2
convert module: layer2.1.bn1
convert module: layer1.2.bn1
convert module: layer2.0.downsample.1
convert module: layer1.1.conv2
convert module: layer2.0.downsample.1
convert module: layer2.1.conv3
convert module: layer1.0.relu
convert module: layer1.2.conv2
convert module: layer2.1.conv1
convert module: layer1.1.bn2
convert module: layer2.1.conv2
convert module: layer2.1.conv1
convert module: layer1.0.downsample.0
convert module: layer1.2.bn2
convert module: layer2.1.bn3
convert module: layer1.1.conv3
convert module: layer2.1.bn1
convert module: layer3.4.bn2
convert module: layer1.0.downsample.1
convert module: layer2.1.bn1
convert module: layer1.2.conv3
convert module: layer1.1.bn3
convert module: layer2.1.conv2
convert module: layer2.1.relu
convert module: layer3.4.conv3
convert module: layer2.1.bn2
convert module: layer2.1.conv2
convert module: layer1.1.conv1
convert module: layer1.2.bn3
convert module: layer1.1.relu
convert module: layer2.1.conv3
convert module: layer1.1.bn1
convert module: layer2.2.conv1
convert module: layer2.1.bn2
convert module: layer1.2.conv1
convert module: layer3.4.bn3
convert module: layer2.1.bn2
convert module: layer1.2.relu
convert module: layer2.1.conv3
convert module: layer1.1.conv2
convert module: layer2.1.bn3
convert module: layer1.2.bn1
convert module: layer2.2.bn1
convert module: layer2.1.conv3
convert module: layer3.4.relu
convert module: layer2.0.conv1
convert module: layer2.1.bn3
convert module: layer1.1.bn2
convert module: layer1.2.conv2
convert module: layer2.1.relu
convert module: layer3.5.conv1
convert module: layer2.1.bn3
convert module: layer2.0.bn1
convert module: layer2.2.conv2
convert module: layer2.1.relu
convert module: layer1.2.bn2
convert module: layer2.2.conv1
convert module: layer1.1.conv3
convert module: layer2.1.relu
convert module: layer2.0.conv2
convert module: layer2.2.conv1
convert module: layer1.2.conv3
convert module: layer1.1.bn3
convert module: layer3.5.bn1
convert module: layer2.2.bn1
convert module: layer2.2.conv1
convert module: layer2.2.bn1
convert module: layer1.2.bn3
convert module: layer1.1.relu
convert module: layer2.2.bn2
convert module: layer2.0.bn2
convert module: layer3.5.conv2
convert module: layer2.2.bn1
convert module: layer1.2.conv1
convert module: layer2.2.conv2
convert module: layer2.2.conv2
convert module: layer2.0.conv3
convert module: layer2.2.conv3
convert module: layer1.2.relu
convert module: layer2.2.conv2
convert module: layer1.2.bn1
convert module: layer2.0.conv1
convert module: layer2.0.bn3
convert module: layer2.2.bn2
convert module: layer2.2.bn3
convert module: layer1.2.conv2
convert module: layer3.5.bn2
convert module: layer2.0.bn1
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer2.0.relu
convert module: layer2.2.conv3
convert module: layer2.2.relu
convert module: layer1.2.bn2
convert module: layer3.5.conv3
convert module: layer2.0.conv2
convert module: layer2.2.conv3
convert module: layer2.0.downsample.0
convert module: layer2.2.conv3
convert module: layer2.3.conv1
convert module: layer2.2.bn3
convert module: layer1.2.conv3
convert module: layer2.2.bn3
convert module: layer2.2.bn3
convert module: layer2.0.downsample.1
convert module: layer2.2.relu
convert module: layer1.2.bn3
convert module: layer2.3.bn1
convert module: layer2.0.bn2
convert module: layer3.5.bn3
convert module: layer2.2.relu
convert module: layer2.3.conv1
convert module: layer2.2.relu
convert module: layer2.1.conv1
convert module: layer3.5.relu
convert module: layer2.3.conv2
convert module: layer2.0.conv3
convert module: layer2.3.conv1
convert module: layer1.2.relu
convert module: layer2.3.bn1
convert module: layer4.0.conv1
convert module: layer2.3.conv1
convert module: layer2.1.bn1
convert module: layer2.0.conv1
convert module: layer2.0.bn3
convert module: layer2.3.bn1
convert module: layer2.3.conv2
convert module: layer2.1.conv2
convert module: layer2.3.bn1
convert module: layer2.0.relu
convert module: layer2.0.bn1
convert module: layer2.3.conv2
convert module: layer2.3.bn2
convert module: layer2.0.downsample.0
convert module: layer4.0.bn1
convert module: layer2.3.bn2
convert module: layer2.3.conv2
convert module: layer2.0.conv2
convert module: layer2.1.bn2
convert module: layer2.3.conv3
convert module: layer4.0.conv2
convert module: layer2.3.conv3
convert module: layer2.3.bn2
convert module: layer2.0.downsample.1
convert module: layer2.1.conv3
convert module: layer2.0.bn2
convert module: layer2.3.bn3
convert module: layer2.3.bn3
convert module: layer2.3.conv3
convert module: layer2.3.bn2
convert module: layer2.1.conv1
convert module: layer2.1.bn3
convert module: layer2.0.conv3
convert module: layer2.3.relu
convert module: layer2.3.relu
convert module: layer2.1.relu
convert module: layer2.3.conv3
convert module: layer2.3.bn3
convert module: layer2.1.bn1
convert module: layer3.0.conv1
convert module: layer2.0.bn3
convert module: layer3.0.conv1
convert module: layer2.2.conv1
convert module: layer2.3.relu
convert module: layer2.1.conv2
convert module: layer2.3.bn3
convert module: layer2.0.relu
convert module: layer3.0.conv1
convert module: layer2.2.bn1
convert module: layer3.0.bn1
convert module: layer2.0.downsample.0
convert module: layer2.3.relu
convert module: layer3.0.bn1
convert module: layer2.1.bn2
convert module: layer4.0.bn2
convert module: layer2.2.conv2
convert module: layer3.0.conv2
convert module: layer3.0.conv1
convert module: layer3.0.bn1
convert module: layer3.0.conv2
convert module: layer2.0.downsample.1
convert module: layer2.1.conv3
convert module: layer4.0.conv3
convert module: layer3.0.conv2
convert module: layer2.2.bn2
convert module: layer2.1.conv1
convert module: layer2.1.bn3
convert module: layer3.0.bn1
convert module: layer2.2.conv3
convert module: layer2.1.relu
convert module: layer2.1.bn1
convert module: layer3.0.bn2
convert module: layer2.2.conv1
convert module: layer3.0.bn2
convert module: layer3.0.conv2
convert module: layer2.2.bn3
convert module: layer2.1.conv2
convert module: layer3.0.conv3
convert module: layer3.0.bn2
convert module: layer4.0.bn3
convert module: layer2.2.relu
convert module: layer2.2.bn1
convert module: layer3.0.conv3
convert module: layer3.0.conv3
convert module: layer4.0.relu
convert module: layer2.1.bn2
convert module: layer2.3.conv1
convert module: layer3.0.bn3
convert module: layer2.2.conv2
convert module: layer4.0.downsample.0
convert module: layer2.1.conv3
convert module: layer2.3.bn1
convert module: layer3.0.relu
convert module: layer3.0.bn3
convert module: layer3.0.bn3
convert module: layer3.0.bn2
convert module: layer2.2.bn2
convert module: layer2.3.conv2
convert module: layer3.0.downsample.0
convert module: layer2.1.bn3
convert module: layer3.0.relu
convert module: layer3.0.relu
convert module: layer3.0.conv3
convert module: layer2.2.conv3
convert module: layer3.0.downsample.0
convert module: layer2.1.relu
convert module: layer2.3.bn2
convert module: layer3.0.downsample.0
convert module: layer2.2.bn3
convert module: layer2.2.conv1
convert module: layer3.0.downsample.1
convert module: layer2.3.conv3
convert module: layer3.0.bn3
convert module: layer2.2.relu
convert module: layer4.0.downsample.1
convert module: layer2.2.bn1
convert module: layer3.1.conv1
convert module: layer2.3.conv1
convert module: layer3.0.downsample.1
convert module: layer3.0.relu
convert module: layer2.3.bn3
convert module: layer3.0.downsample.1
convert module: layer4.1.conv1
convert module: layer2.2.conv2
convert module: layer2.3.relu
convert module: layer3.0.downsample.0
convert module: layer3.1.conv1
convert module: layer2.3.bn1
convert module: layer3.1.conv1
convert module: layer3.1.bn1
convert module: layer3.0.conv1
convert module: layer2.3.conv2
convert module: layer2.2.bn2
convert module: layer3.1.conv2
convert module: layer3.1.bn1
convert module: layer2.2.conv3
convert module: layer3.0.bn1
convert module: layer2.3.bn2
convert module: layer3.1.conv2
convert module: layer3.1.bn1
convert module: layer3.0.conv2
convert module: layer2.2.bn3
convert module: layer4.1.bn1
convert module: layer2.3.conv3
convert module: layer3.0.downsample.1
convert module: layer2.2.relu
convert module: layer3.1.bn2
convert module: layer4.1.conv2
convert module: layer3.1.conv2
convert module: layer2.3.bn3
convert module: layer2.3.conv1
convert module: layer3.1.conv1
convert module: layer3.1.conv3
convert module: layer3.1.bn2
convert module: layer2.3.relu
convert module: layer3.0.bn2
convert module: layer2.3.bn1
convert module: layer3.0.conv1
convert module: layer3.1.conv3
convert module: layer3.0.conv3
convert module: layer3.1.bn2
convert module: layer2.3.conv2
convert module: layer3.1.bn3
convert module: layer3.1.bn1
convert module: layer3.0.bn1
convert module: layer3.1.relu
convert module: layer3.1.conv3
convert module: layer3.1.conv2
convert module: layer3.1.bn3
convert module: layer3.0.bn3
convert module: layer3.2.conv1
convert module: layer3.0.conv2
convert module: layer4.1.bn2
convert module: layer2.3.bn2
convert module: layer3.0.relu
convert module: layer3.1.relu
convert module: layer3.0.downsample.0
convert module: layer4.1.conv3
convert module: layer3.1.bn3
convert module: layer3.2.conv1
convert module: layer2.3.conv3
convert module: layer3.2.bn1
convert module: layer3.1.bn2
convert module: layer3.2.conv2
convert module: layer3.0.bn2
convert module: layer2.3.bn3
convert module: layer3.1.relu
convert module: layer3.0.conv3
convert module: layer3.2.bn1
convert module: layer3.2.conv1
convert module: layer2.3.relu
convert module: layer3.1.conv3
convert module: layer3.0.downsample.1
convert module: layer3.0.conv1
convert module: layer3.2.conv2
convert module: layer4.1.bn3
convert module: layer3.1.conv1
convert module: layer3.2.bn2
convert module: layer3.0.bn3
convert module: layer4.1.relu
convert module: layer3.0.bn1
convert module: layer3.2.bn1
convert module: layer3.2.conv3
convert module: layer3.0.relu
convert module: layer3.1.bn3
convert module: layer4.2.conv1
convert module: layer3.0.downsample.0
convert module: layer3.1.bn1
convert module: layer3.2.bn2
convert module: layer3.0.conv2
convert module: layer3.2.conv2
convert module: layer3.1.relu
convert module: layer3.1.conv2
convert module: layer3.2.bn3
convert module: layer3.2.conv3
convert module: layer3.2.conv1
convert module: layer3.2.relu
convert module: layer3.0.downsample.1
convert module: layer3.3.conv1
convert module: layer4.2.bn1
convert module: layer3.2.bn2
convert module: layer3.2.bn3
convert module: layer3.0.bn2
convert module: layer3.1.conv1
convert module: layer4.2.conv2
convert module: layer3.1.bn2
convert module: layer3.2.conv3
convert module: layer3.2.relu
convert module: layer3.0.conv3
convert module: layer3.2.bn1
convert module: layer3.3.bn1
convert module: layer3.1.conv3
convert module: layer3.3.conv1
convert module: layer3.1.bn1
convert module: layer3.3.conv2
convert module: layer3.2.conv2
convert module: layer3.0.bn3
convert module: layer3.2.bn3
convert module: layer3.1.conv2
convert module: layer3.1.bn3
convert module: layer3.3.bn1
convert module: layer3.0.relu
convert module: layer3.1.relu
convert module: layer3.0.downsample.0
convert module: layer3.3.conv2
convert module: layer3.2.relu
convert module: layer3.2.conv1
convert module: layer3.3.bn2
convert module: layer3.3.conv1
convert module: layer3.2.bn2
convert module: layer4.2.bn2
convert module: layer3.3.conv3
convert module: layer3.1.bn2
convert module: layer4.2.conv3
convert module: layer3.2.conv3
convert module: layer3.2.bn1
convert module: layer3.0.downsample.1
convert module: layer3.3.bn2
convert module: layer3.1.conv3
convert module: layer3.3.bn1
convert module: layer3.2.conv2
convert module: layer3.3.bn3
convert module: layer3.1.conv1
convert module: layer3.3.conv3
convert module: layer3.1.bn3
convert module: layer3.3.relu
convert module: layer3.3.conv2
convert module: layer3.2.bn3
convert module: layer3.4.conv1
convert module: layer3.1.relu
convert module: layer4.2.bn3
convert module: layer3.3.bn3
convert module: layer3.1.bn1
convert module: layer3.2.bn2
convert module: layer3.2.conv1
convert module: layer3.2.relu
convert module: layer4.2.relu
convert module: layer3.3.relu
convert module: layer3.4.bn1
convert module: layer3.1.conv2
convert module: layer3.3.conv1
convert module: layer3.2.conv3
convert module: layer3.3.bn2
convert module: layer3.4.conv1
convert module: avgpool
convert module: layer3.2.bn1
convert module: fc
convert module: layer3.3.conv3
convert module: layer3.4.conv2
convert module: layer3.2.bn3
convert module: layer3.2.conv2
convert module: layer3.1.bn2
convert module: layer3.3.bn1
convert module: layer3.4.bn1
convert module: layer3.2.relu
convert module: layer3.3.bn3
convert module: layer3.1.conv3
convert module: layer3.4.conv2
convert module: layer3.3.conv1
convert module: layer3.4.bn2
convert module: layer3.3.conv2
convert module: layer3.3.relu
convert module: layer3.2.bn2
convert module: layer3.1.bn3
convert module: layer3.4.conv1
convert module: layer3.4.conv3
convert module: layer3.3.bn1
convert module: layer3.2.conv3
convert module: layer3.1.relu
convert module: layer3.4.bn2
convert module: layer3.3.conv2
convert module: layer3.2.conv1
convert module: layer3.4.bn3
convert module: layer3.4.conv3
convert module: layer3.3.bn2
convert module: layer3.2.bn3
convert module: layer3.4.bn1
convert module: layer3.4.relu
convert module: layer3.2.relu
convert module: layer3.3.conv3
convert module: layer3.5.conv1
convert module: layer3.2.bn1
convert module: layer3.4.conv2
convert module: layer3.4.bn3
convert module: layer3.3.conv1
convert module: layer3.3.bn2
convert module: layer3.2.conv2
convert module: layer3.4.relu
convert module: layer3.3.conv3
convert module: layer3.5.bn1
convert module: layer3.3.bn3
convert module: layer3.5.conv1
convert module: layer3.3.bn1
convert module: layer3.5.conv2
convert module: layer3.4.bn2
convert module: layer3.3.conv2
convert module: layer3.3.relu
convert module: layer3.3.bn3
convert module: layer3.2.bn2
convert module: layer3.5.bn1
convert module: layer3.4.conv3
convert module: layer3.4.conv1
convert module: layer3.3.relu
convert module: layer3.2.conv3
convert module: layer3.5.conv2
convert module: layer3.4.conv1
convert module: layer3.5.bn2
convert module: layer3.4.bn3
convert module: layer3.3.bn2
convert module: layer3.5.conv3
convert module: layer3.2.bn3
convert module: layer3.4.bn1
convert module: layer3.3.conv3
convert module: layer3.2.relu
convert module: layer3.4.relu
convert module: layer3.4.bn1
convert module: layer3.5.bn2
convert module: layer3.5.conv1
convert module: layer3.5.bn3
convert module: layer3.3.conv1
convert module: layer3.4.conv2
convert module: layer3.4.conv2
convert module: layer3.5.conv3
convert module: layer3.3.bn3
convert module: layer3.5.relu
convert module: layer4.0.conv1
convert module: layer3.3.relu
convert module: layer3.5.bn1
convert module: layer3.3.bn1
convert module: layer3.5.bn3
convert module: layer3.4.conv1
convert module: layer3.3.conv2
convert module: layer3.4.bn2
convert module: layer3.4.bn2
convert module: layer3.5.conv2
convert module: layer3.5.relu
convert module: layer3.4.conv3
convert module: layer4.0.bn1
convert module: layer4.0.conv1
convert module: layer3.4.conv3
convert module: layer3.4.bn1
convert module: layer4.0.conv2
convert module: layer3.3.bn2
convert module: layer3.4.conv2
convert module: layer3.4.bn3
convert module: layer3.5.bn2
convert module: layer3.4.bn3
convert module: layer4.0.bn1
convert module: layer3.3.conv3
convert module: layer3.4.relu
convert module: layer3.5.conv3
convert module: layer4.0.conv2
convert module: layer3.5.conv1
convert module: layer3.4.relu
convert module: layer3.3.bn3
convert module: layer3.4.bn2
convert module: layer3.5.conv1
convert module: layer3.3.relu
convert module: layer3.5.bn3
convert module: layer3.5.bn1
convert module: layer3.4.conv3
convert module: layer3.4.conv1
convert module: layer3.5.conv2
convert module: layer3.5.relu
convert module: layer3.5.bn1
convert module: layer3.4.bn3
convert module: layer4.0.conv1
convert module: layer3.4.relu
convert module: layer3.4.bn1
convert module: layer4.0.bn2
convert module: layer4.0.bn2
convert module: layer3.5.conv2
convert module: layer3.5.conv1
convert module: layer3.5.bn2
convert module: layer3.4.conv2
convert module: layer4.0.conv3
convert module: layer4.0.conv3
convert module: layer3.5.conv3
convert module: layer4.0.bn1
convert module: layer3.5.bn1
convert module: layer4.0.conv2
convert module: layer3.5.conv2
convert module: layer3.5.bn3
convert module: layer3.4.bn2
convert module: layer3.5.bn2
convert module: layer4.0.bn3
convert module: layer3.5.relu
convert module: layer3.4.conv3
convert module: layer4.0.bn3
convert module: layer3.5.conv3
convert module: layer4.0.conv1
convert module: layer4.0.relu
convert module: layer3.5.bn2
convert module: layer4.0.relu
convert module: layer4.0.downsample.0
convert module: layer3.4.bn3
convert module: layer4.0.downsample.0
convert module: layer3.5.conv3
convert module: layer3.5.bn3
convert module: layer3.4.relu
convert module: layer4.0.bn1
convert module: layer3.5.conv1
convert module: layer3.5.relu
convert module: layer3.5.bn3
convert module: layer4.0.conv2
convert module: layer4.0.bn2
convert module: layer4.0.conv1
convert module: layer3.5.relu
convert module: layer3.5.bn1
convert module: layer4.0.conv3
convert module: layer4.0.conv1
convert module: layer3.5.conv2
convert module: layer4.0.downsample.1
convert module: layer4.1.conv1
convert module: layer4.0.bn1
convert module: layer4.0.bn1
convert module: layer4.0.bn3
convert module: layer4.0.downsample.1
convert module: layer3.5.bn2
convert module: layer4.0.conv2
convert module: layer4.0.conv2
convert module: layer3.5.conv3
convert module: layer4.1.conv1
convert module: layer4.0.relu
convert module: layer4.0.downsample.0
convert module: layer4.1.bn1
convert module: layer4.0.bn2
convert module: layer3.5.bn3
convert module: layer4.1.conv2
convert module: layer4.0.conv3
convert module: layer3.5.relu
convert module: layer4.0.conv1
convert module: layer4.1.bn1
convert module: layer4.1.conv2
convert module: layer4.0.downsample.1
convert module: layer4.0.bn1
convert module: layer4.0.bn3
convert module: layer4.0.bn2
convert module: layer4.0.conv2
convert module: layer4.1.conv1
convert module: layer4.0.bn2
convert module: layer4.1.bn2
convert module: layer4.0.relu
convert module: layer4.0.conv3
convert module: layer4.0.downsample.0
convert module: layer4.1.conv3
convert module: layer4.0.conv3
convert module: layer4.1.bn1
convert module: layer4.0.bn3
convert module: layer4.1.bn3
convert module: layer4.1.conv2
convert module: layer4.1.bn2
convert module: layer4.0.relu
convert module: layer4.0.bn3
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.1.conv3
convert module: layer4.0.downsample.0
convert module: layer4.0.relu
convert module: layer4.0.bn2
convert module: layer4.0.downsample.1
convert module: layer4.0.downsample.0
convert module: layer4.0.conv3
convert module: layer4.1.conv1
convert module: layer4.2.bn1
convert module: layer4.1.bn3
convert module: layer4.1.bn2
convert module: layer4.2.conv2
convert module: layer4.1.relu
convert module: layer4.1.conv3
convert module: layer4.2.conv1
convert module: layer4.0.bn3
convert module: layer4.1.bn1
convert module: layer4.0.downsample.1
convert module: layer4.0.relu
convert module: layer4.1.conv2
convert module: layer4.0.downsample.0
convert module: layer4.1.conv1
convert module: layer4.0.downsample.1
convert module: layer4.1.bn3
convert module: layer4.2.bn2
convert module: layer4.2.bn1
convert module: layer4.1.conv1
convert module: layer4.2.conv3
convert module: layer4.2.conv2
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.1.bn1
convert module: layer4.1.conv2
convert module: layer4.2.bn3
convert module: layer4.0.downsample.1
convert module: layer4.2.relu
convert module: layer4.1.bn2
convert module: layer4.2.bn1
convert module: layer4.1.bn1
convert module: layer4.1.conv1
convert module: avgpool
convert module: layer4.1.conv3
convert module: fc
convert module: layer4.2.conv2
convert module: layer4.1.conv2
convert module: layer4.2.bn2
convert module: layer4.2.conv3
convert module: layer4.1.bn1
convert module: layer4.1.bn3
convert module: layer4.1.bn2
convert module: layer4.1.conv2
convert module: layer4.1.relu
convert module: layer4.2.bn2
convert module: layer4.1.conv3
convert module: layer4.2.conv1
convert module: layer4.2.conv3
convert module: layer4.2.bn3
convert module: layer4.1.bn2
convert module: layer4.2.relu
convert module: avgpool
convert module: layer4.1.conv3
convert module: fc
convert module: layer4.1.bn3
convert module: layer4.2.bn1
convert module: layer4.2.bn3
convert module: layer4.1.relu
convert module: layer4.2.conv2
convert module: layer4.2.conv1
convert module: layer4.2.relu
convert module: layer4.1.bn2
convert module: avgpool
convert module: fc
convert module: layer4.1.conv3
convert module: layer4.1.bn3
convert module: layer4.1.relu
convert module: layer4.2.bn1
convert module: layer4.2.conv1
convert module: layer4.2.conv2
convert module: layer4.1.bn3
convert module: layer4.2.bn2
convert module: layer4.2.conv3
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.2.bn1
convert module: layer4.2.conv2
convert module: layer4.2.bn3
convert module: layer4.2.bn2
convert module: layer4.2.relu
convert module: layer4.2.bn1
convert module: layer4.2.conv3
convert module: avgpool
convert module: layer4.2.conv2
convert module: fc
convert module: layer4.2.bn3
convert module: layer4.2.bn2
convert module: layer4.2.relu
convert module: avgpool
convert module: layer4.2.conv3
convert module: fc
convert module: layer4.2.bn2
convert module: layer4.2.conv3
convert module: layer4.2.bn3
convert module: layer4.2.relu
convert module: avgpool
convert module: fc
convert module: layer4.2.bn3
convert module: layer4.2.relu
convert module: avgpool
convert module: fc
2020-07-02 15:09:06,061 [32mINFO[0m: model
DistributedParrotsModel
ResNet(
  (conv1): Conv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (bn1): BatchNorm2d(
    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (relu): ReLU(
    inplace
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (maxpool): MaxPool2d(
    kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (4): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (5): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (avgpool): AvgPool2d(
    kernel_size=7, stride=1, padding=0
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (fc): Linear(
    in_features=2048, out_features=1000, bias=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
)[0m
2020-07-02 15:09:06,160 [32mINFO[0m: loss
CrossEntropyLoss()[0m
2020-07-02 15:09:06,162 [32mINFO[0m: optimizer
SGD (
Parameter Group 0
    dampening: 0
    hold_grads: False
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)[0m
2020-07-02 15:09:30,301 [32mINFO[0m: Epoch: [1/100][   0/5005] Time 22.590 (22.590) Data 18.048 (18.048) Loss 7.0487 (7.0487) Acc@1 0.00 (0.00) Acc@5 0.00 (0.00) Memory(MB) 7067[0m
2020-07-02 15:09:40,349 [32mINFO[0m: Epoch: [1/100][  20/5005] Time 0.489 (1.554) Data 0.001 (0.861) Loss 8.2141 (8.0525) Acc@1 0.00 (0.04) Acc@5 0.39 (0.35) Memory(MB) 7695[0m
2020-07-02 15:09:50,813 [32mINFO[0m: Epoch: [1/100][  40/5005] Time 0.468 (1.051) Data 0.001 (0.442) Loss 7.7397 (8.1238) Acc@1 0.00 (0.07) Acc@5 0.00 (0.42) Memory(MB) 7695[0m
2020-07-02 15:10:00,481 [32mINFO[0m: Epoch: [1/100][  60/5005] Time 0.466 (0.865) Data 0.002 (0.298) Loss 7.0756 (7.8938) Acc@1 0.00 (0.07) Acc@5 0.00 (0.41) Memory(MB) 7695[0m
2020-07-02 15:10:11,490 [32mINFO[0m: Epoch: [1/100][  80/5005] Time 0.562 (0.787) Data 0.002 (0.226) Loss 6.9795 (7.3587) Acc@1 1.17 (0.08) Acc@5 1.95 (0.48) Memory(MB) 7695[0m
2020-07-02 15:10:21,395 [32mINFO[0m: Epoch: [1/100][ 100/5005] Time 0.461 (0.729) Data 0.001 (0.181) Loss 6.9418 (7.0354) Acc@1 0.39 (0.13) Acc@5 0.78 (0.54) Memory(MB) 7695[0m
2020-07-02 15:10:31,481 [32mINFO[0m: Epoch: [1/100][ 120/5005] Time 0.469 (0.692) Data 0.002 (0.152) Loss 6.9597 (6.9485) Acc@1 0.78 (0.13) Acc@5 1.95 (0.62) Memory(MB) 7695[0m
2020-07-02 15:10:41,329 [32mINFO[0m: Epoch: [1/100][ 140/5005] Time 0.477 (0.664) Data 0.001 (0.131) Loss 6.9228 (6.9275) Acc@1 0.39 (0.12) Acc@5 0.39 (0.59) Memory(MB) 7695[0m
2020-07-02 15:10:51,742 [32mINFO[0m: Epoch: [1/100][ 160/5005] Time 0.462 (0.646) Data 0.001 (0.115) Loss 6.9297 (6.9217) Acc@1 0.00 (0.12) Acc@5 0.00 (0.59) Memory(MB) 7695[0m
2020-07-02 15:11:01,684 [32mINFO[0m: Epoch: [1/100][ 180/5005] Time 0.503 (0.630) Data 0.001 (0.102) Loss 6.9055 (6.9155) Acc@1 0.00 (0.16) Acc@5 0.78 (0.73) Memory(MB) 7695[0m
