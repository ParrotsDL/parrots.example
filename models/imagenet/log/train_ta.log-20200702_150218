--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           SH-IDC1-10-198-4-158
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:26,187 [32mINFO[0m: Parrots 0.6.0 | Git hash: c59bcc36 | Compute hash: b4b006c0[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 5 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 0 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 2 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 6 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 7 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 4 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 1 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:28,189 [32mINFO[0m: rank 3 of 8 jobs, in SH-IDC1-10-198-4-158[0m
2020-07-02 15:02:34,661 [32mINFO[0m: config
{
  "seed": 99,
  "net": {
    "arch": "resnet50",
    "kwargs": {
      "num_classes": 1000
    }
  },
  "dataset": {
    "train": {
      "meta_file": "/mnt/lustre/share/images/meta/train.txt",
      "image_dir": "/mnt/lustre/share/images/train",
      "random_resize_crop": 224,
      "colorjitter": [
        0.2,
        0.2,
        0.2,
        0.1
      ],
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": true
    },
    "test": {
      "meta_file": "/mnt/lustre/share/images/meta/val.txt",
      "image_dir": "/mnt/lustre/share/images/val",
      "resize": 256,
      "center_crop": [
        224,
        224
      ],
      "colorjitter": null,
      "mean": [
        0.485,
        0.456,
        0.406
      ],
      "std": [
        0.229,
        0.224,
        0.225
      ],
      "mirror": false
    },
    "batch_size": 32,
    "workers": 4
  },
  "trainer": {
    "max_epoch": 100,
    "test_freq": 1,
    "log_freq": 20,
    "bn": {
      "syncbn": false
    },
    "mixed_precision": {
      "half": false,
      "loss_scale": 128.0,
      "float_bn": true
    },
    "optimizer": {
      "type": "SGD",
      "kwargs": {
        "lr": 0.1,
        "momentum": 0.9,
        "weight_decay": 0.0001
      }
    },
    "lr_scheduler": {
      "warmup_epochs": 0,
      "type": "MultiStepLR",
      "kwargs": {
        "milestones": [
          30,
          60,
          90
        ],
        "gamma": 0.1
      }
    }
  },
  "saver": {
    "pretrain_model": null,
    "resume_model": null,
    "save_dir": "checkpoints/resnet50"
  },
  "monitor": {
    "type": "pavi",
    "_taskid": null,
    "kwargs": {
      "project": "pape_test",
      "task": "resnet50",
      "model": "resnet50"
    }
  }
}[0m
2020-07-02 15:02:35,017 [32mINFO[0m: creating model 'resnet50'[0m
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: conv1
convert module: bn1
convert module: relu
convert module: maxpool
convert module: layer1.0.conv1
convert module: bn1
convert module: layer1.0.bn1
convert module: relu
convert module: layer1.0.conv2
convert module: maxpool
convert module: layer1.0.bn2
convert module: layer1.0.conv1
convert module: layer1.0.conv3
convert module: layer1.0.bn1
convert module: layer1.0.bn3
convert module: layer1.0.conv2
convert module: layer1.0.relu
convert module: layer1.0.bn2
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.1
convert module: layer1.0.conv3
convert module: layer1.0.bn3
convert module: layer1.1.conv1
convert module: layer1.0.relu
convert module: layer1.1.bn1
convert module: layer1.0.downsample.0
convert module: layer1.1.conv2
convert module: layer1.0.downsample.1
convert module: layer1.1.bn2
convert module: layer1.1.conv1
convert module: layer1.1.conv3
convert module: layer1.1.bn1
convert module: layer1.1.bn3
convert module: layer1.1.conv2
convert module: layer1.1.relu
convert module: layer1.2.conv1
convert module: layer1.1.bn2
convert module: layer1.1.conv3
convert module: layer1.2.bn1
convert module: layer1.1.bn3
convert module: layer1.2.conv2
convert module: layer1.1.relu
convert module: layer1.2.bn2
convert module: layer1.2.conv1
convert module: layer1.2.conv3
convert module: layer1.2.bn1
convert module: layer1.2.bn3
convert module: layer1.2.conv2
convert module: layer1.2.relu
convert module: layer1.2.bn2
convert module: layer2.0.conv1
convert module: layer1.2.conv3
convert module: layer2.0.bn1
convert module: layer1.2.bn3
convert module: layer2.0.conv2
convert module: layer1.2.relu
convert module: layer2.0.conv1
convert module: bn1
convert module: layer2.0.bn2
convert module: layer2.0.bn1
convert module: layer2.0.conv3
convert module: layer2.0.conv2
convert module: relu
convert module: layer2.0.bn3
convert module: layer2.0.bn2
convert module: maxpool
convert module: layer2.0.relu
convert module: layer2.0.conv3
convert module: layer1.0.conv1
convert module: bn1
convert module: layer2.0.downsample.0
convert module: layer2.0.bn3
convert module: layer1.0.bn1
convert module: layer2.0.relu
convert module: layer2.0.downsample.1
convert module: layer2.0.downsample.0
convert module: relu
convert module: layer1.0.conv2
convert module: layer2.1.conv1
convert module: maxpool
convert module: layer2.0.downsample.1
convert module: layer1.0.bn2
convert module: layer1.0.conv1
convert module: layer2.1.bn1
convert module: layer2.1.conv1
convert module: layer1.0.conv3
convert module: layer1.0.bn1
convert module: layer2.1.conv2
convert module: layer2.1.bn1
convert module: layer1.0.bn3
convert module: layer1.0.conv2
convert module: layer2.1.conv2
convert module: layer2.1.bn2
convert module: layer1.0.relu
convert module: layer1.0.bn2
convert module: layer2.1.conv3
convert module: layer2.1.bn2
convert module: layer1.0.downsample.0
convert module: layer1.0.conv3
convert module: layer2.1.conv3
convert module: layer2.1.bn3
convert module: layer1.0.downsample.1
convert module: layer1.0.bn3
convert module: layer2.1.bn3
convert module: layer2.1.relu
convert module: layer1.0.relu
convert module: layer2.1.relu
convert module: layer1.1.conv1
convert module: layer2.2.conv1
convert module: layer1.0.downsample.0
convert module: layer2.2.conv1
convert module: layer2.2.bn1
convert module: layer1.1.bn1
convert module: layer1.0.downsample.1
convert module: layer2.2.bn1
convert module: layer1.1.conv2
convert module: layer2.2.conv2
convert module: layer1.1.conv1
convert module: layer2.2.conv2
convert module: layer1.1.bn2
convert module: layer1.1.bn1
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer1.1.conv2
convert module: layer1.1.conv3
convert module: layer2.2.conv3
convert module: layer2.2.conv3
convert module: layer1.1.bn2
convert module: layer1.1.bn3
convert module: layer2.2.bn3
convert module: layer2.2.bn3
convert module: layer1.1.conv3
convert module: layer2.2.relu
convert module: layer2.2.relu
convert module: layer1.1.relu
convert module: layer1.1.bn3
convert module: layer2.3.conv1
convert module: layer2.3.conv1
convert module: layer1.2.conv1
convert module: layer1.1.relu
convert module: layer2.3.bn1
convert module: layer2.3.bn1
convert module: layer1.2.bn1
convert module: layer1.2.conv1
convert module: layer2.3.conv2
convert module: bn1
convert module: layer2.3.conv2
convert module: layer1.2.bn1
convert module: layer1.2.conv2
convert module: layer2.3.bn2
convert module: layer1.2.conv2
convert module: layer1.2.bn2
convert module: layer2.3.bn2
convert module: layer2.3.conv3
convert module: layer1.2.bn2
convert module: layer1.2.conv3
convert module: layer2.3.conv3
convert module: relu
convert module: layer2.3.bn3
convert module: layer1.2.conv3
convert module: maxpool
convert module: layer1.2.bn3
convert module: layer2.3.bn3
convert module: layer2.3.relu
convert module: layer1.2.bn3
convert module: layer2.3.relu
convert module: layer3.0.conv1
convert module: layer1.0.conv1
convert module: layer1.2.relu
convert module: layer3.0.conv1
convert module: layer2.0.conv1
convert module: layer1.0.bn1
convert module: layer1.2.relu
convert module: layer3.0.bn1
convert module: layer2.0.conv1
convert module: layer1.0.conv2
convert module: layer3.0.bn1
convert module: layer3.0.conv2
convert module: layer2.0.bn1
convert module: bn1
convert module: layer2.0.bn1
convert module: layer3.0.conv2
convert module: layer2.0.conv2
convert module: layer1.0.bn2
convert module: layer2.0.conv2
convert module: layer1.0.conv3
convert module: relu
convert module: layer3.0.bn2
convert module: layer2.0.bn2
convert module: layer1.0.bn3
convert module: layer2.0.bn2
convert module: maxpool
convert module: layer3.0.bn2
convert module: layer2.0.conv3
convert module: layer3.0.conv3
convert module: layer3.0.conv3
convert module: layer2.0.conv3
convert module: layer1.0.relu
convert module: layer1.0.conv1
convert module: layer2.0.bn3
convert module: layer1.0.downsample.0
convert module: layer3.0.bn3
convert module: layer2.0.bn3
convert module: layer1.0.bn1
convert module: layer2.0.relu
convert module: layer3.0.bn3
convert module: layer1.0.downsample.1
convert module: layer3.0.relu
convert module: layer2.0.downsample.0
convert module: layer2.0.relu
convert module: layer3.0.relu
convert module: layer1.0.conv2
convert module: layer3.0.downsample.0
convert module: layer2.0.downsample.0
convert module: layer1.1.conv1
convert module: layer3.0.downsample.0
convert module: layer1.0.bn2
convert module: bn1
convert module: layer2.0.downsample.1
convert module: layer1.1.bn1
convert module: layer2.0.downsample.1
convert module: layer3.0.downsample.1
convert module: layer1.0.conv3
convert module: bn1
convert module: layer2.1.conv1
convert module: layer3.0.downsample.1
convert module: layer2.1.conv1
convert module: layer3.1.conv1
convert module: layer1.1.conv2
convert module: layer1.0.bn3
convert module: layer3.1.conv1
convert module: layer2.1.bn1
convert module: layer1.1.bn2
convert module: relu
convert module: layer2.1.bn1
convert module: layer1.0.relu
convert module: layer2.1.conv2
convert module: layer3.1.bn1
convert module: layer1.1.conv3
convert module: layer2.1.conv2
convert module: maxpool
convert module: layer1.0.downsample.0
convert module: layer3.1.bn1
convert module: relu
convert module: layer3.1.conv2
convert module: layer1.1.bn3
convert module: layer2.1.bn2
convert module: layer3.1.conv2
convert module: layer1.0.conv1
convert module: maxpool
convert module: layer2.1.bn2
convert module: layer1.0.downsample.1
convert module: layer1.0.conv1
convert module: layer1.1.relu
convert module: layer2.1.conv3
convert module: layer2.1.conv3
convert module: layer1.0.bn1
convert module: layer1.2.conv1
convert module: layer1.1.conv1
convert module: layer3.1.bn2
convert module: layer1.0.bn1
convert module: layer2.1.bn3
convert module: layer1.0.conv2
convert module: layer3.1.bn2
convert module: layer2.1.bn3
convert module: layer3.1.conv3
convert module: layer1.0.conv2
convert module: layer2.1.relu
convert module: layer1.2.bn1
convert module: layer1.1.bn1
convert module: layer3.1.conv3
convert module: layer2.1.relu
convert module: layer1.0.bn2
convert module: layer1.0.bn2
convert module: layer2.2.conv1
convert module: layer1.1.conv2
convert module: layer1.2.conv2
convert module: layer2.2.conv1
convert module: layer3.1.bn3
convert module: layer1.0.conv3
convert module: layer3.1.bn3
convert module: layer1.0.conv3
convert module: layer1.1.bn2
convert module: layer1.2.bn2
convert module: layer2.2.bn1
convert module: layer3.1.relu
convert module: layer2.2.bn1
convert module: layer1.0.bn3
convert module: layer3.1.relu
convert module: layer1.0.bn3
convert module: layer3.2.conv1
convert module: layer1.2.conv3
convert module: layer1.1.conv3
convert module: layer3.2.conv1
convert module: layer2.2.conv2
convert module: layer1.0.relu
convert module: layer2.2.conv2
convert module: layer1.0.relu
convert module: layer1.0.downsample.0
convert module: layer1.0.downsample.0
convert module: layer1.2.bn3
convert module: layer1.1.bn3
convert module: layer3.2.bn1
convert module: layer3.2.bn1
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer1.0.downsample.1
convert module: layer1.0.downsample.1
convert module: layer1.1.relu
convert module: layer3.2.conv2
convert module: layer3.2.conv2
convert module: layer2.2.conv3
convert module: layer1.2.relu
convert module: layer1.2.conv1
convert module: layer1.1.conv1
convert module: layer2.2.conv3
convert module: layer1.1.conv1
convert module: layer2.0.conv1
convert module: layer1.1.bn1
convert module: layer2.2.bn3
convert module: layer1.1.bn1
convert module: layer1.2.bn1
convert module: layer2.2.bn3
convert module: layer3.2.bn2
convert module: layer2.0.bn1
convert module: layer1.1.conv2
convert module: layer2.2.relu
convert module: layer3.2.bn2
convert module: layer1.1.conv2
convert module: layer2.2.relu
convert module: layer1.2.conv2
convert module: layer3.2.conv3
convert module: layer1.1.bn2
convert module: layer2.3.conv1
convert module: layer3.2.conv3
convert module: layer2.0.conv2
convert module: layer2.3.conv1
convert module: layer1.2.bn2
convert module: layer1.1.conv3
convert module: layer1.1.bn2
convert module: layer2.3.bn1
convert module: layer3.2.bn3
convert module: layer1.2.conv3
convert module: layer2.3.bn1
convert module: layer1.1.bn3
convert module: layer2.0.bn2
convert module: layer1.1.conv3
convert module: layer3.2.bn3
convert module: layer3.2.relu
convert module: layer2.3.conv2
convert module: layer2.3.conv2
convert module: layer1.1.relu
convert module: layer3.2.relu
convert module: layer2.0.conv3
convert module: layer1.2.bn3
convert module: layer1.1.bn3
convert module: layer3.3.conv1
convert module: layer1.2.conv1
convert module: layer3.3.conv1
convert module: layer1.1.relu
convert module: layer2.3.bn2
convert module: layer2.3.bn2
convert module: layer2.0.bn3
convert module: layer1.2.bn1
convert module: layer1.2.relu
convert module: layer1.2.conv1
convert module: layer3.3.bn1
convert module: layer2.3.conv3
convert module: layer2.0.conv1
convert module: layer1.2.conv2
convert module: layer3.3.bn1
convert module: layer2.3.conv3
convert module: layer2.0.relu
convert module: layer3.3.conv2
convert module: layer1.2.bn1
convert module: layer3.3.conv2
convert module: layer1.2.bn2
convert module: layer2.0.downsample.0
convert module: layer2.3.bn3
convert module: layer2.3.bn3
convert module: layer2.0.bn1
convert module: layer1.2.conv2
convert module: layer1.2.conv3
convert module: layer2.3.relu
convert module: layer2.3.relu
convert module: layer2.0.conv2
convert module: layer2.0.downsample.1
convert module: layer3.3.bn2
convert module: layer1.2.bn3
convert module: layer1.2.bn2
convert module: layer3.0.conv1
convert module: layer3.0.conv1
convert module: layer3.3.bn2
convert module: layer3.3.conv3
convert module: layer2.1.conv1
convert module: layer1.2.conv3
convert module: layer1.2.relu
convert module: layer3.3.conv3
convert module: layer2.0.bn2
convert module: layer3.0.bn1
convert module: layer3.0.bn1
convert module: layer2.0.conv1
convert module: layer1.2.bn3
convert module: layer2.1.bn1
convert module: layer3.3.bn3
convert module: layer2.0.conv3
convert module: layer3.0.conv2
convert module: layer3.0.conv2
convert module: layer1.2.relu
convert module: layer2.0.bn1
convert module: layer3.3.bn3
convert module: layer2.1.conv2
convert module: layer3.3.relu
convert module: layer2.0.conv1
convert module: layer2.0.conv2
convert module: layer3.3.relu
convert module: layer3.4.conv1
convert module: layer2.0.bn3
convert module: layer2.1.bn2
convert module: layer3.4.conv1
convert module: layer3.0.bn2
convert module: layer2.0.bn2
convert module: layer2.0.bn1
convert module: layer3.0.bn2
convert module: layer2.0.relu
convert module: layer2.1.conv3
convert module: layer3.0.conv3
convert module: layer3.4.bn1
convert module: layer2.0.conv3
convert module: layer2.0.downsample.0
convert module: layer2.0.conv2
convert module: layer3.0.conv3
convert module: layer3.4.bn1
convert module: layer3.4.conv2
convert module: layer2.1.bn3
convert module: layer2.0.bn3
convert module: layer3.0.bn3
convert module: layer3.4.conv2
convert module: layer2.1.relu
convert module: layer2.0.bn2
convert module: layer2.0.relu
convert module: layer2.0.downsample.1
convert module: layer3.0.bn3
convert module: layer3.0.relu
convert module: layer2.0.downsample.0
convert module: layer2.2.conv1
convert module: layer3.0.downsample.0
convert module: layer3.4.bn2
convert module: layer2.0.conv3
convert module: layer3.0.relu
convert module: layer2.1.conv1
convert module: layer3.4.conv3
convert module: layer3.0.downsample.0
convert module: layer2.0.downsample.1
convert module: layer2.0.bn3
convert module: layer3.4.bn2
convert module: layer2.2.bn1
convert module: layer2.1.bn1
convert module: layer2.1.conv1
convert module: layer3.4.conv3
convert module: layer3.0.downsample.1
convert module: layer2.0.relu
convert module: layer2.2.conv2
convert module: layer2.1.conv2
convert module: layer3.4.bn3
convert module: layer2.1.bn1
convert module: layer2.0.downsample.0
convert module: layer3.1.conv1
convert module: layer3.4.relu
convert module: layer3.0.downsample.1
convert module: layer3.4.bn3
convert module: layer2.1.conv2
convert module: layer2.2.bn2
convert module: layer3.5.conv1
convert module: layer2.1.bn2
convert module: layer3.1.conv1
convert module: layer3.4.relu
convert module: layer2.0.downsample.1
convert module: layer3.1.bn1
convert module: layer2.2.conv3
convert module: layer3.5.conv1
convert module: layer2.1.bn2
convert module: layer2.1.conv1
convert module: layer2.1.conv3
convert module: layer3.1.conv2
convert module: layer3.5.bn1
convert module: layer2.1.conv3
convert module: layer3.1.bn1
convert module: layer2.2.bn3
convert module: layer3.5.conv2
convert module: layer2.1.bn1
convert module: layer2.1.bn3
convert module: layer3.5.bn1
convert module: layer2.1.bn3
convert module: layer2.2.relu
convert module: layer3.1.conv2
convert module: layer3.5.conv2
convert module: layer2.1.relu
convert module: layer2.1.conv2
convert module: layer3.1.bn2
convert module: layer2.1.relu
convert module: layer2.3.conv1
convert module: layer2.2.conv1
convert module: layer2.2.conv1
convert module: layer3.1.conv3
convert module: layer3.5.bn2
convert module: layer2.1.bn2
convert module: layer2.3.bn1
convert module: layer2.2.bn1
convert module: layer3.5.conv3
convert module: layer2.2.bn1
convert module: layer3.5.bn2
convert module: layer2.1.conv3
convert module: layer2.3.conv2
convert module: layer3.1.bn3
convert module: layer3.1.bn2
convert module: layer2.2.conv2
convert module: layer2.2.conv2
convert module: layer3.5.conv3
convert module: layer3.1.relu
convert module: layer3.5.bn3
convert module: layer2.1.bn3
convert module: layer3.1.conv3
convert module: layer2.3.bn2
convert module: layer3.2.conv1
convert module: layer3.5.relu
convert module: layer2.2.bn2
convert module: layer2.2.bn2
convert module: layer3.5.bn3
convert module: layer2.1.relu
convert module: layer2.3.conv3
convert module: layer4.0.conv1
convert module: layer2.2.conv3
convert module: layer2.2.conv1
convert module: layer3.1.bn3
convert module: layer3.5.relu
convert module: layer3.2.bn1
convert module: layer2.2.conv3
convert module: layer4.0.conv1
convert module: layer2.2.bn3
convert module: layer2.2.bn1
convert module: layer2.3.bn3
convert module: layer3.2.conv2
convert module: layer3.1.relu
convert module: layer2.2.bn3
convert module: layer4.0.bn1
convert module: layer2.2.relu
convert module: layer2.3.relu
convert module: layer3.2.conv1
convert module: layer2.2.conv2
convert module: layer2.2.relu
convert module: layer2.3.conv1
convert module: layer4.0.conv2
convert module: layer3.0.conv1
convert module: layer4.0.bn1
convert module: layer3.2.bn2
convert module: layer2.3.conv1
convert module: layer2.3.bn1
convert module: layer2.2.bn2
convert module: layer4.0.conv2
convert module: layer3.2.bn1
convert module: layer3.2.conv3
convert module: layer2.3.conv2
convert module: layer2.3.bn1
convert module: layer2.2.conv3
convert module: layer3.0.bn1
convert module: layer3.2.conv2
convert module: layer2.3.conv2
convert module: layer2.2.bn3
convert module: layer3.0.conv2
convert module: layer2.3.bn2
convert module: layer3.2.bn3
convert module: layer2.3.conv3
convert module: layer3.2.relu
convert module: layer2.2.relu
convert module: layer4.0.bn2
convert module: layer2.3.bn2
convert module: layer3.3.conv1
convert module: layer2.3.bn3
convert module: layer3.2.bn2
convert module: layer2.3.conv1
convert module: layer3.0.bn2
convert module: layer4.0.conv3
convert module: layer2.3.relu
convert module: layer3.2.conv3
convert module: layer2.3.conv3
convert module: layer2.3.bn1
convert module: layer3.0.conv3
convert module: layer3.3.bn1
convert module: layer3.0.conv1
convert module: layer2.3.conv2
convert module: layer2.3.bn3
convert module: layer3.3.conv2
convert module: layer3.2.bn3
convert module: layer4.0.bn2
convert module: layer3.0.bn1
convert module: layer2.3.relu
convert module: layer4.0.bn3
convert module: layer3.0.bn3
convert module: layer4.0.conv3
convert module: layer3.2.relu
convert module: layer2.3.bn2
convert module: layer3.0.conv2
convert module: layer3.0.conv1
convert module: layer4.0.relu
convert module: layer3.0.relu
convert module: layer3.3.bn2
convert module: layer2.3.conv3
convert module: layer3.3.conv1
convert module: layer3.0.downsample.0
convert module: layer4.0.downsample.0
convert module: layer3.3.conv3
convert module: layer2.3.bn3
convert module: layer3.0.bn2
convert module: layer3.0.bn1
convert module: layer3.3.bn1
convert module: layer3.0.conv3
convert module: layer2.3.relu
convert module: layer3.3.bn3
convert module: layer3.0.conv2
convert module: layer4.0.bn3
convert module: layer3.3.conv2
convert module: layer3.0.downsample.1
convert module: layer3.0.conv1
convert module: layer3.3.relu
convert module: layer4.0.relu
convert module: layer3.0.bn3
convert module: layer3.1.conv1
convert module: layer3.4.conv1
convert module: layer4.0.downsample.0
convert module: layer4.0.downsample.1
convert module: layer3.0.relu
convert module: layer3.0.bn1
convert module: layer3.0.bn2
convert module: layer3.0.downsample.0
convert module: layer4.1.conv1
convert module: layer3.1.bn1
convert module: layer3.3.bn2
convert module: layer3.0.conv2
convert module: layer3.4.bn1
convert module: layer3.0.conv3
convert module: layer3.1.conv2
convert module: layer3.3.conv3
convert module: layer3.4.conv2
convert module: layer3.0.downsample.1
convert module: layer3.1.conv1
convert module: layer3.0.bn2
convert module: layer3.3.bn3
convert module: layer3.0.bn3
convert module: layer4.1.bn1
convert module: layer3.4.bn2
convert module: layer3.0.conv3
convert module: layer3.1.bn2
convert module: layer3.3.relu
convert module: layer3.0.relu
convert module: layer4.1.conv2
convert module: layer3.1.bn1
convert module: layer3.4.conv3
convert module: layer4.0.downsample.1
convert module: layer3.0.downsample.0
convert module: layer3.1.conv3
convert module: layer3.4.conv1
convert module: layer3.1.conv2
convert module: layer4.1.conv1
convert module: layer3.0.bn3
convert module: layer3.4.bn3
convert module: layer3.0.relu
convert module: layer3.1.bn3
convert module: layer3.4.relu
convert module: layer3.4.bn1
convert module: layer3.0.downsample.1
convert module: layer3.1.bn2
convert module: layer3.0.downsample.0
convert module: layer3.1.relu
convert module: layer3.5.conv1
convert module: layer3.4.conv2
convert module: layer3.1.conv3
convert module: layer3.1.conv1
convert module: layer3.2.conv1
convert module: layer4.1.bn2
convert module: layer4.1.bn1
convert module: layer3.5.bn1
convert module: layer4.1.conv3
convert module: layer3.0.downsample.1
convert module: layer3.1.bn3
convert module: layer3.1.bn1
convert module: layer3.2.bn1
convert module: layer4.1.conv2
convert module: layer3.5.conv2
convert module: layer3.4.bn2
convert module: layer3.1.relu
convert module: layer3.1.conv1
convert module: layer3.1.conv2
convert module: layer3.2.conv2
convert module: layer3.2.conv1
convert module: layer3.4.conv3
convert module: layer3.5.bn2
convert module: layer4.1.bn3
convert module: layer3.2.bn1
convert module: layer3.1.bn1
convert module: layer3.4.bn3
convert module: layer3.5.conv3
convert module: layer3.1.bn2
convert module: layer4.1.relu
convert module: layer3.2.bn2
convert module: layer3.2.conv2
convert module: layer3.1.conv2
convert module: layer4.2.conv1
convert module: layer3.4.relu
convert module: layer3.1.conv3
convert module: layer3.5.bn3
convert module: layer3.2.conv3
convert module: layer3.5.conv1
convert module: layer3.5.relu
convert module: layer3.2.bn2
convert module: layer3.1.bn3
convert module: layer3.1.bn2
convert module: layer4.0.conv1
convert module: layer4.1.bn2
convert module: layer3.2.bn3
convert module: layer3.5.bn1
convert module: layer3.2.conv3
convert module: layer4.2.bn1
convert module: layer3.1.relu
convert module: layer3.1.conv3
convert module: layer4.1.conv3
convert module: layer3.2.relu
convert module: layer3.5.conv2
convert module: layer4.2.conv2
convert module: layer3.2.conv1
convert module: layer4.0.bn1
convert module: layer3.2.bn3
convert module: layer3.3.conv1
convert module: layer3.1.bn3
convert module: layer4.0.conv2
convert module: layer3.2.relu
convert module: layer3.2.bn1
convert module: layer3.1.relu
convert module: layer3.3.conv1
convert module: layer3.3.bn1
convert module: layer3.2.conv2
convert module: layer3.5.bn2
convert module: layer4.1.bn3
convert module: layer3.2.conv1
convert module: layer3.3.conv2
convert module: layer3.3.bn1
convert module: layer4.1.relu
convert module: layer3.5.conv3
convert module: layer4.2.bn2
convert module: layer4.2.conv1
convert module: layer3.3.conv2
convert module: layer3.2.bn1
convert module: layer4.2.conv3
convert module: layer3.2.bn2
convert module: layer3.5.bn3
convert module: layer4.0.bn2
convert module: layer3.2.conv2
convert module: layer3.3.bn2
convert module: layer3.2.conv3
convert module: layer4.0.conv3
convert module: layer3.5.relu
convert module: layer3.3.bn2
convert module: layer3.3.conv3
convert module: layer4.0.conv1
convert module: layer3.3.conv3
convert module: layer3.2.bn2
convert module: layer3.2.bn3
convert module: layer4.2.bn1
convert module: layer4.2.bn3
convert module: layer3.3.bn3
convert module: layer3.2.conv3
convert module: layer3.2.relu
convert module: layer4.2.conv2
convert module: layer4.2.relu
convert module: layer3.3.bn3
convert module: layer4.0.bn3
convert module: avgpool
convert module: layer4.0.bn1
convert module: layer3.3.relu
convert module: layer3.3.conv1
convert module: fc
convert module: layer3.3.relu
convert module: layer4.0.relu
convert module: layer4.0.conv2
convert module: layer3.4.conv1
convert module: layer3.4.conv1
convert module: layer3.2.bn3
convert module: layer4.0.downsample.0
convert module: layer3.3.bn1
convert module: layer3.2.relu
convert module: layer3.3.conv1
convert module: layer3.3.conv2
convert module: layer3.4.bn1
convert module: layer3.4.bn1
convert module: layer3.4.conv2
convert module: layer3.4.conv2
convert module: layer3.3.bn1
convert module: layer4.0.downsample.1
convert module: layer4.2.bn2
convert module: layer3.3.bn2
convert module: layer3.3.conv2
convert module: layer3.4.bn2
convert module: layer4.1.conv1
convert module: layer3.4.bn2
convert module: layer4.2.conv3
convert module: layer3.3.conv3
convert module: layer3.4.conv3
convert module: layer4.0.bn2
convert module: layer3.4.conv3
convert module: layer3.3.bn2
convert module: layer3.3.bn3
convert module: layer3.4.bn3
convert module: layer4.0.conv3
convert module: layer3.3.conv3
convert module: layer3.4.bn3
convert module: layer3.4.relu
convert module: layer3.3.relu
convert module: layer4.1.bn1
convert module: layer4.2.bn3
convert module: layer3.5.conv1
convert module: layer3.4.relu
convert module: layer3.4.conv1
convert module: layer4.1.conv2
convert module: layer4.2.relu
convert module: layer3.3.bn3
convert module: layer3.5.conv1
convert module: avgpool
convert module: layer3.5.bn1
convert module: layer3.3.relu
convert module: fc
convert module: layer4.0.bn3
convert module: layer3.5.conv2
convert module: layer3.4.conv1
convert module: layer3.4.bn1
convert module: layer3.5.bn1
convert module: layer4.0.relu
convert module: layer3.4.conv2
convert module: layer3.5.conv2
convert module: layer4.0.downsample.0
convert module: layer3.5.bn2
convert module: layer3.4.bn1
convert module: layer4.1.bn2
convert module: layer3.5.conv3
convert module: layer3.4.conv2
convert module: layer4.1.conv3
convert module: layer3.5.bn2
convert module: layer3.4.bn2
convert module: layer3.5.bn3
convert module: layer3.5.conv3
convert module: layer3.4.conv3
convert module: layer3.5.relu
convert module: layer3.4.bn2
convert module: layer4.0.conv1
convert module: layer3.5.bn3
convert module: layer3.4.conv3
convert module: layer3.4.bn3
convert module: layer4.1.bn3
convert module: layer3.5.relu
convert module: layer3.4.relu
convert module: layer4.1.relu
convert module: layer4.0.downsample.1
convert module: layer4.0.bn1
convert module: layer4.2.conv1
convert module: layer4.0.conv1
convert module: layer3.4.bn3
convert module: layer3.5.conv1
convert module: layer4.1.conv1
convert module: layer4.0.conv2
convert module: layer3.4.relu
convert module: layer3.5.conv1
convert module: layer3.5.bn1
convert module: layer4.0.bn1
convert module: layer4.2.bn1
convert module: layer3.5.conv2
convert module: layer4.0.conv2
convert module: layer4.2.conv2
convert module: layer3.5.bn1
convert module: layer4.0.bn2
convert module: layer4.1.bn1
convert module: layer3.5.conv2
convert module: layer3.5.bn2
convert module: layer4.0.conv3
convert module: layer4.1.conv2
convert module: layer3.5.conv3
convert module: layer3.5.bn2
convert module: layer4.2.bn2
convert module: layer4.0.bn2
convert module: layer3.5.conv3
convert module: layer4.0.bn3
convert module: layer3.5.bn3
convert module: layer4.2.conv3
convert module: layer4.0.conv3
convert module: layer4.0.relu
convert module: layer3.5.relu
convert module: layer4.0.downsample.0
convert module: layer3.5.bn3
convert module: layer4.0.conv1
convert module: layer3.5.relu
convert module: layer4.2.bn3
convert module: layer4.0.conv1
convert module: layer4.0.bn3
convert module: layer4.2.relu
convert module: layer4.0.bn1
convert module: layer4.1.bn2
convert module: avgpool
convert module: layer4.0.conv2
convert module: layer4.0.relu
convert module: layer4.1.conv3
convert module: fc
convert module: layer4.0.downsample.1
convert module: layer4.0.bn1
convert module: layer4.0.downsample.0
convert module: layer4.1.conv1
convert module: layer4.0.conv2
convert module: layer4.1.bn3
convert module: layer4.1.bn1
convert module: layer4.0.bn2
convert module: layer4.1.relu
convert module: layer4.0.downsample.1
convert module: layer4.2.conv1
convert module: layer4.1.conv2
convert module: layer4.0.conv3
convert module: layer4.1.conv1
convert module: layer4.0.bn2
convert module: layer4.0.conv3
convert module: layer4.0.bn3
convert module: layer4.2.bn1
convert module: layer4.1.bn1
convert module: layer4.1.bn2
convert module: layer4.0.relu
convert module: layer4.0.bn3
convert module: layer4.2.conv2
convert module: layer4.1.conv2
convert module: layer4.1.conv3
convert module: layer4.0.downsample.0
convert module: layer4.0.relu
convert module: layer4.0.downsample.0
convert module: layer4.1.bn3
convert module: layer4.1.relu
convert module: layer4.0.downsample.1
convert module: layer4.2.conv1
convert module: layer4.1.bn2
convert module: layer4.1.conv1
convert module: layer4.1.conv3
convert module: layer4.0.downsample.1
convert module: layer4.2.bn2
convert module: layer4.1.conv1
convert module: layer4.2.conv3
convert module: layer4.2.bn1
convert module: layer4.2.conv2
convert module: layer4.1.bn3
convert module: layer4.1.bn1
convert module: layer4.1.relu
convert module: layer4.1.bn1
convert module: layer4.1.conv2
convert module: layer4.2.conv1
convert module: layer4.2.bn3
convert module: layer4.1.conv2
convert module: layer4.2.relu
convert module: avgpool
convert module: layer4.2.bn2
convert module: fc
convert module: layer4.2.conv3
convert module: layer4.2.bn1
convert module: layer4.2.conv2
convert module: layer4.1.bn2
convert module: layer4.1.bn2
convert module: layer4.1.conv3
convert module: layer4.2.bn3
convert module: layer4.1.conv3
convert module: layer4.2.relu
convert module: avgpool
convert module: fc
convert module: layer4.1.bn3
convert module: layer4.2.bn2
convert module: layer4.1.bn3
convert module: layer4.1.relu
convert module: layer4.1.relu
convert module: layer4.2.conv1
convert module: layer4.2.conv3
convert module: layer4.2.conv1
convert module: layer4.2.bn3
convert module: layer4.2.bn1
convert module: layer4.2.bn1
convert module: layer4.2.relu
convert module: layer4.2.conv2
convert module: layer4.2.conv2
convert module: avgpool
convert module: fc
convert module: layer4.2.bn2
convert module: layer4.2.bn2
convert module: layer4.2.conv3
convert module: layer4.2.conv3
convert module: layer4.2.bn3
convert module: layer4.2.bn3
convert module: layer4.2.relu
convert module: layer4.2.relu
convert module: avgpool
convert module: avgpool
convert module: fc
convert module: fc
2020-07-02 15:02:57,131 [32mINFO[0m: model
DistributedParrotsModel
ResNet(
  (conv1): Conv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (bn1): BatchNorm2d(
    64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (relu): ReLU(
    inplace
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (maxpool): MaxPool2d(
    kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (3): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (4): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (5): Bottleneck(
      (conv1): Conv2d(
        1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(
        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (downsample): Sequential(
        (0): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
        (1): BatchNorm2d(
          2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
          (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
        )
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
    (2): Bottleneck(
      (conv1): Conv2d(
        2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn1): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn2): BatchNorm2d(
        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (conv3): Conv2d(
        512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (bn3): BatchNorm2d(
        2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
      (relu): ReLU(
        inplace
        (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
      )
    )
  )
  (avgpool): AvgPool2d(
    kernel_size=7, stride=1, padding=0
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
  (fc): Linear(
    in_features=2048, out_features=1000, bias=True
    (output_quant): FakeLinearQuantization(mode=ASYMMETRIC_UNSIGNED, num_bits=8, ema_decay=0.9990))
  )
)[0m
2020-07-02 15:02:57,234 [32mINFO[0m: loss
CrossEntropyLoss()[0m
2020-07-02 15:02:57,235 [32mINFO[0m: optimizer
SGD (
Parameter Group 0
    dampening: 0
    hold_grads: False
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)[0m
2020-07-02 15:02:59,891 ERROR: new_task error [NotLoginError: not login]
2020-07-02 15:02:59,899 [31mERROR[0m: Uncaught exception: 
Traceback (most recent call last):
  File "main.py", line 313, in <module>
    main()
  File "main.py", line 161, in main
    session_text=yaml.dump(args.config), **cfgs.monitor.kwargs)
  File "/mnt/cache/share/platform/env/miniconda3.6/envs/pat0.6.0rc0/lib/python3.6/site-packages/pavi/writer.py", line 89, in __init__
    raise _WriterCreationError()
pavi.exception.WriterCreationError: create writer error[0m
Traceback (most recent call last):
  File "main.py", line 313, in <module>
    main()
  File "main.py", line 161, in main
    session_text=yaml.dump(args.config), **cfgs.monitor.kwargs)
  File "/mnt/cache/share/platform/env/miniconda3.6/envs/pat0.6.0rc0/lib/python3.6/site-packages/pavi/writer.py", line 89, in __init__
    raise _WriterCreationError()
pavi.exception.WriterCreationError: create writer error
