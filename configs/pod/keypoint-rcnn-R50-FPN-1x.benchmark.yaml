dataset: # Required.
  type: coco
  class_names: ["__background__", "person"]
  train:
    meta_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/person_keypoints_train2017.json
    image_dir: /mnt/lustre/share/DSK/datasets/mscoco2017/train2017
    ceph_image_dir: s3://parrots_model_data/DSK/datasets/mscoco2017/train2017
    flip: True
    scales: [640, 672, 704, 736, 768, 800] # shorter side of resized image
    max_size: 1333        # longer side of resized image
  test:
    meta_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/person_keypoints_val2017.json
    image_dir: /mnt/lustre/share/DSK/datasets/mscoco2017/val2017
    ceph_image_dir: s3://parrots_model_data/DSK/datasets/mscoco2017/val2017
    flip: False
    scales: [800]         # shorter side of resized image
    max_size: 1333        # longer side of resized image
  has_keypoint: True      # don't use keypoints
  has_mask: False         # don't use segmentations
  #aspect_grouping: [1, ] # do not use group sampler for keypoint
  alignment: 32           # Align size of images to fit FPN. For fpn-faster-rcnn, 32 is enough; for RetinaNet, it's 128.
  pixel_mean: [0.485, 0.456, 0.406] # ImageNet pretrained statics
  pixel_std: [0.229, 0.224, 0.225]
  batch_size: 2
  workers: 4              # number of workers of dataloader for each process
  evaluator:
    type: COCO
    kwargs:
      gt_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/person_keypoints_val2017.json
      iou_types: [bbox, keypoints] # calculate mAP_bbox & mAP_keyp

trainer: # Required.
  max_epoch: 0       # total epochs for the training
  test_freq: 1
  optimizer:          # optimizer = SGD(params,lr=0.001,momentum=0.9,weight_decay=0.0001)
    type: SGD
    kwargs:
      lr: 0.00125
      momentum: 0.9
      weight_decay: 0.0001
  lr_scheduler:       # lr_scheduler = MultStepLR(optimizer, milestones=[9,14],gamma=0.1)
    warmup_epochs: 1  # set to be 0 to disable warmup. target_lr = init_lr * total_batch_size
    type: MultiStepLR
    kwargs:
      milestones: [13,20]   # epochs to decay lr
      gamma: 0.1            # decay rate

saver: # Required.
  save_dir: checkpoints/keypoint-rcnn-R50-FPN-1x     # dir to save checkpoints
  pretrain_model: /mnt/lustre/share/DSK/model_zoo/pytorch/imagenet/resnet50-19c8e357.pth 
  results_dir: results_dir/keypoint-rcnn-R50-FPN-1x  # dir to save detection results. i.e., bboxes, masks, keypoints
  #vis_gt:
  #  output_dir: vis_gt
  #vis_dt:
  #  output_dir: vis_dt
  #  bbox_thresh: 0.3
  #  keyp_thresh: 0.01


#fp16:
#  keep_batchnorm_fp32: True

net: # Required.
  - name: backbone              # backbone = resnet50(frozen_layers, out_layers, out_strides)
    type: pod.models.backbones.resnet.resnet50
    kwargs:
      frozen_layers: [0, 1]     # layer0...1 is fixed
      out_layers: [1,2,3,4]     # layer1...4, commonly named Conv2...5
      out_strides: [4,8,16,32]  # tell the strides of output features
      deformable: False         # set it as true if using deform for fpn
      normalize:
        type: freeze_bn
      # layer4:                 # provide config for layer4 if using deform for fpn, do not use atrous' algorithm
      #   stride: 2
      #   dilation: 1
      #   block: DeformBlock
  - name: neck
    prev: backbone
    type: pod.models.necks.fpn.FPN
    kwargs:
      outplanes: 256
      start_level: 2
      num_level: 5                # if num_level>len(backbone.out_layers), additional conv with be stacked.
      out_strides: [4,8,16,32,64] # strides of output features. aka., anchor strides for roi_head
      downsample: pool            # method to downsample, for FPN, it's pool, for RetienaNet, it's conv
      upsample: nearest           # method to interp, nearest or bilinear
      initializer:
        method: xavier
  - name: roi_head
    prev: neck
    type: pod.models.heads.roi_head.NaiveRPN
    kwargs:
      feat_planes: 256    # channels of intermediate conv
      num_classes: 2      # for rpn, it's 2; for RetinaNet, it's 81
      initializer:
        method: normal
        std: 0.01
      cfg:
        cls_loss: softmax
        anchor_ratios: [0.5,1,2]  # anchor strides are provided as feature strides by feature extractor
        anchor_scales: [8]        # scale of anchors relative to feature map
        nms:
          type: naive             # use vanilla nms
          nms_iou_thresh: 0.7     # nms thresh. Within each level if FPN
        roi_min_size: 0           # minimum scale of a valid roi
        allowed_border: 0
        train:
          matcher:
            ignore_iou_thresh: 0.5        # Required if provide ignore_regions
            positive_iou_thresh: 0.7
            negative_iou_thresh: 0.3
            allow_low_quality_match: True # train only. positive if a anchor has highest iou with any gt 
          sampler:
            type: 'naive'
            batch_size: 256               # batch size of sampled anchors of each image
            positive_percent: 0.5         # percent of positives in sampled anchors
          pre_nms_score_thresh: 0.00      # to reduce computation
          pre_nms_top_n: 2000     # number of rois before nms. (within each level if fpn)
          post_nms_top_n: 2000    # number of rois after nms. (within each level if fpn)
          across_levels:          # merge multi-level proposals
            nms:
              type: naive
              nms_iou_thresh: -1  # Required in FPN, nms across levels.
            top_n: 2000           # number of rois to keep after merging rois from all levels
        test:
          pre_nms_score_thresh: 0.0       # to reduce computation
          pre_nms_top_n: 1000
          post_nms_top_n: 1000
          across_levels:
            nms:
              type: naive
              nms_iou_thresh: -1  # Required in FPN, nms across levels.
            top_n: 1000           # number of rois to keep after merging rois from all levels
  - name: bbox_head
    prev: neck
    type: pod.models.heads.bbox_head.FC
    kwargs:
      feat_planes: 1024
      num_classes: 2              # number of classification classes
      initializer:
        method: msra
      cfg:
        smooth_l1_sigma: 1.0      # set as 1000.0 to use L1-loss for bbox head
        fpn:
          fpn_levels: [0,1,2,3]   # indices of fpn features used for this stage. these levels are supposed to be continuous
          base_scale: 56          # target level of a RoI is floor(log2((w*h)**0.5/base_scale))
        roipooling:
          method: 'roialignpool'  # choices=['roialignpool', 'psroipool', 'roipool', 'psroimaskpool']. note that 'psroipool' is for RFCN head
          pool_size: 7
          sampling_ratio: 2
        bbox_normalize:
          means: [0, 0, 0, 0]     # statics to normalize localization predictions.
          stds: [0.1, 0.1, 0.2, 0.2]
        share_location: False     # is share location in bbox regression for all classes
        train:
          matcher:
            ignore_iou_thresh: 0.5          # Required if provide ignore_regions
            positive_iou_thresh: 0.5
            negative_iou_thresh: 0.5
            allow_low_quality_match: False  # positive if a anchor has highest iou with any gt 
          sampler:
            type: 'naive'
            batch_size: 512                 # rois batch size of each image
            positive_percent: 0.25
        test:
          nms:
            type: naive                     # choices = {'naive', 'soft'}
            nms_iou_thresh: 0.5
          bbox_score_thresh: 0.0
          top_n: 100                # test only, number of bboxes to keep
  - name: keyp_head
    prev: neck
    type: pod.models.heads.keyp_head.ConvUp
    kwargs:
      num_keypoints: 17             # number of keypoints
      feat_planes: 512
      num_convs: 8
      deconv_kernel: 4
      initializer:
        method: msra
      cfg:
        fpn:
          fpn_levels: [0,1,2,3]     # indices of fpn features used for this task. these levels are supposed to be continuous
          base_scale: 56            # target level of an roi is np.floor(np.log2((w*h)**0.5/base_scale+eps))
        roipooling:
          method: 'roialignpool'    # choices=['roialignpool', 'psroipool', 'roipool']. note that 'psroipool' is for RFCN head
          pool_size: 14 
          sampling_ratio: 2
        train:
          resample: True            # sampling from rpn proposals; False to use bbox_head sampled results, may be faster
          sampler: # train only
            type: 'naive'
            batch_size: 128           # train only. keypoints batch size of each image
            positive_percent: 1.0
          matcher:
            positive_iou_thresh: 0.5  # train only.
            negative_iou_thresh: -1   # train only
            ignore_iou_thresh: -1
            allow_low_quality_match: False # train only. positive if a anchor has highest iou with any gt 
          label_h: 56         # train only. output size of heatmap
          label_w: 56         # train only.
