dataset: # Required.
  type: coco
  train:
    meta_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/instances_train2017.json
    image_dir: /mnt/lustre/share/DSK/datasets/mscoco2017/train2017
    ceph_image_dir: s3://parrots_model_data/DSK/datasets/mscoco2017/train2017
    flip: True
    scales: [800]              # shorter side of resized image
    max_size: 1333             # longer side of resized image
  test:
    meta_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/instances_val2017.json
    image_dir: /mnt/lustre/share/DSK/datasets/mscoco2017/val2017
    ceph_image_dir: s3://parrots_model_data/DSK/datasets/mscoco2017/val2017
    flip: False
    scales: [800]              # shorter side of resized image
    max_size: 1333             # longer side of resized image
  num_classes: 81
  has_keypoint: False        # don't use keypoints
  has_mask: False            # don't use segmentations
  aspect_grouping: [1, ]     # use group_sampler when loading dataset
  alignment: 32              # Align size of images to fit FPN. For fpn-faster-rcnn, 32 is enough; for RetinaNet, it's 128.
  pixel_mean: [0.485, 0.456, 0.406] # ImageNet pretrained statics
  pixel_std: [0.229, 0.224, 0.225]
  batch_size: 2
  workers: 4                 # number of workers of dataloader for each process
  evaluator:
    type: COCO               # choices = {'COCO', 'VOC', 'mAP'} 
    kwargs:
      gt_file: /mnt/lustre/share/DSK/datasets/mscoco2017/annotations/instances_val2017.json
      iou_types: [bbox]      # calcuate AP_bbox only

trainer: # Required.
  max_epoch: 0              # total epochs for the training
  test_freq: 1
  optimizer:                 # optimizer = SGD(params,lr=0.001,momentum=0.9,weight_decay=0.0001)
    type: SGD
    kwargs:
      lr: 0.00125
      momentum: 0.9
      weight_decay: 0.0001
  lr_scheduler:              # lr_scheduler = MultStepLR(optimizer, milestones=[9,14],gamma=0.1)
    warmup_epochs: 1         # set to be 0 to disable warmup. When warmup,  target_lr = init_lr * total_batch_size
    type: MultiStepLR
    kwargs:
      milestones: [8,11]     # epochs to decay lr
      gamma: 0.1             # decay rate

saver: # Required.
  save_dir: checkpoints/cascade-rcnn-R50-FPN-1x      # dir to save checkpoints
  pretrain_model: /mnt/lustre/share/DSK/model_zoo/pytorch/imagenet/resnet50-19c8e357.pth
  results_dir: results_dir/cascade-rcnn-R50-FPN-1x   # dir to save detection results. i.e., bboxes, masks, keypoints
  #vis_gt:
  #  output_dir: vis_gt
  #vis_dt:
  #  output_dir: vis_dt
  #  bbox_thresh: 0.3


#fp16:
#  keep_batchnorm_fp32: True

net: # Required.
  - name: backbone              # backbone = resnet50(frozen_layers, out_layers, out_strides)
    type: pod.models.backbones.resnet.resnet50
    kwargs:
      frozen_layers: [0, 1]     # layer0...1 is fixed
      out_layers: [1,2,3,4]     # layer1...4, commonly named Conv2...5
      out_strides: [4,8,16,32]  # tell the strides of output features
      deformable: False         # set it as true if using deform for fpn
      normalize:
        type: sync_bn
        kwargs:
          bn_group_size: 8
      # layer4:                 # provide config for layer4 if using deform for fpn, do not use atrous' algorithm
      #   stride: 2
      #   dilation: 1
      #   block: DeformBlock
  - name: neck
    prev: backbone
    type: pod.models.necks.fpn.FPN
    kwargs:
      outplanes: 256
      start_level: 2
      num_level: 5                # if num_level>len(backbone.out_layers), additional conv with be stacked.
      out_strides: [4,8,16,32,64] # strides of output features. aka., anchor strides for roi_head
      downsample: pool            # method to downsample, for FPN, it's pool, for RetienaNet, it's conv
      upsample: nearest           # method to interp, nearest or bilinear
      initializer:
        method: xavier
  - name: roi_head
    prev: neck
    type: pod.models.heads.roi_head.NaiveRPN
    kwargs:
      feat_planes: 256    # channels of intermediate conv
      num_classes: 2      # for rpn, it's 2; for RetinaNet, it's 81
      initializer:
        method: normal
        std: 0.01
      cfg:
        cls_loss: softmax         # use softmax cls loss
        anchor_ratios: [0.5,1,2]  # anchor strides are provided as feature strides by feature extractor
        anchor_scales: [8]        # scale of anchors relative to feature map
        nms:
          type: naive
          nms_iou_thresh: 0.7     # nms thresh. Within each level if FPN
        roi_min_size: 0           # minimum scale of a valid roi
        allowed_border: 0
        train:
          matcher: # train only
            ignore_iou_thresh: 0.5        # train only. Required if provide ignore_regions
            positive_iou_thresh: 0.7      # train only
            negative_iou_thresh: 0.3      # train only
            allow_low_quality_match: True # train only. positive if a anchor has highest iou with any gt 
          sampler: # train only
            type: 'naive'
            batch_size: 256           # train only, batch size of sampled anchors of each image
            positive_percent: 0.5     # train only, number of positives in sampled anchors
          pre_nms_score_thresh: 0.00  # to reduce computation
          pre_nms_top_n: 2000         # number of rois before nms. (within each level if fpn)
          post_nms_top_n: 2000        # number of rois after nms. (within each level if fpn)
          across_levels:              # merge multi-level proposals
            nms:
              type: naive
              nms_iou_thresh: -1      # Required in RetinaNet. DO not nms in FPN across levels 
            top_n: 2000               # number of rois to keep after merging rois from all levels
        test:
          pre_nms_score_thresh: 0.0   # to reduce computation
          pre_nms_top_n: 1000
          post_nms_top_n: 1000
          across_levels:              # merge multi-level proposals
            nms:
              type: naive
              nms_iou_thresh: -1      # Required in RetinaNet. DO not nms in FPN across levels 
            top_n: 1000               # number of rois to keep after merging rois from all levels
  - name: bbox_head
    prev: neck
    type: pod.models.heads.cascade_head.FC
    kwargs:
      feat_planes: 1024
      num_classes: 81             # number of classification classes
      initializer:
        method: msra
      cfg:
        smooth_l1_sigma: 1.0      # set as 1000.0 to use L1-loss for bbox head
        fpn:
          fpn_levels: [0,1,2,3]   # indices of fpn features used for this stage. these levels are supposed to be continuous
          base_scale: 56          # target level of a RoI is floor(log2((w*h)**0.5/base_scale))
        roipooling:
          method: 'roialignpool'  # choices=['roialignpool', 'psroipool', 'roipool']. note that 'psroipool' is for RFCN head
          pool_size: 7
          sampling_ratio: 2
        num_stage: 3
        stage_weights: [1, 0.5, 0.25]
        test_stage: 3
        test_ensemble: True
        stage_bbox_normalize:
          means: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]     # statics to normalize localization predictions.
          stds: [[0.1, 0.1, 0.2, 0.2], [0.05, 0.05, 0.1, 0.1], [0.033, 0.033, 0.067, 0.067]]
        share_location: True     # is share location in bbox regression for all classes
        train:
          stage_matcher: # train only
            ignore_iou_thresh: 0.5          # train only. Required if provide ignore_regions
            positive_iou_thresh: [0.5, 0.6, 0.7]        # train only
            negative_iou_thresh: [0.5, 0.6, 0.7]        # train only
            allow_low_quality_match: False  # train only. positive if a anchor has highest iou with any gt 
          sampler: # train only
            type: 'naive'
            batch_size: 512         # train only, rois batch size of each image
            positive_percent: 0.25  # train only
        test:
          nms:
            type: naive
            nms_iou_thresh: 0.5     # test only
          bbox_score_thresh: 0.0    # test only
          top_n: 100                # test only, number of bboxes to keep
